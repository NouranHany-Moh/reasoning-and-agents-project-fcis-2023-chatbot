{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMvjThxJwqge"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pickle\n",
        "import json\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from keras import models\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWlUpeXoxDhm",
        "outputId": "0d77f2ea-5e5f-421f-d42f-24bc57539179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(['Hi'], 'greeting'), (['How', 'are', 'you', '?'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['What', \"'s\", 'up'], 'greeting'), (['how', 'are', 'ya'], 'greeting'), (['heyy'], 'greeting'), (['whatsup'], 'greeting'), (['?', '?', '?', '?', '?', '?', '?', '?'], 'greeting'), (['cya'], 'goodbye'), (['see', 'you'], 'goodbye'), (['bye', 'bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['I', 'am', 'Leaving'], 'goodbye'), (['Bye'], 'goodbye'), (['Have', 'a', 'Good', 'day'], 'goodbye'), (['talk', 'to', 'you', 'later'], 'goodbye'), (['ttyl'], 'goodbye'), (['i', 'got', 'to', 'go'], 'goodbye'), (['gtg'], 'goodbye'), (['what', 'is', 'the', 'name', 'of', 'your', 'developers'], 'creator'), (['what', 'is', 'the', 'name', 'of', 'your', 'creators'], 'creator'), (['what', 'is', 'the', 'name', 'of', 'the', 'developers'], 'creator'), (['what', 'is', 'the', 'name', 'of', 'the', 'creators'], 'creator'), (['who', 'created', 'you'], 'creator'), (['your', 'developers'], 'creator'), (['your', 'creators'], 'creator'), (['who', 'are', 'your', 'developers'], 'creator'), (['developers'], 'creator'), (['you', 'are', 'made', 'by'], 'creator'), (['you', 'are', 'made', 'by', 'whom'], 'creator'), (['who', 'created', 'you'], 'creator'), (['who', 'create', 'you'], 'creator'), (['creators'], 'creator'), (['who', 'made', 'you'], 'creator'), (['who', 'designed', 'you'], 'creator'), (['name'], 'name'), (['your', 'name'], 'name'), (['do', 'you', 'have', 'a', 'name'], 'name'), (['what', 'are', 'you', 'called'], 'name'), (['what', 'is', 'your', 'name'], 'name'), (['what', 'should', 'I', 'call', 'you'], 'name'), (['whats', 'your', 'name', '?'], 'name'), (['what', 'are', 'you'], 'name'), (['who', 'are', 'you'], 'name'), (['who', 'is', 'this'], 'name'), (['what', 'am', 'i', 'chatting', 'to'], 'name'), (['who', 'am', 'i', 'taking', 'to'], 'name'), (['what', 'are', 'you'], 'name'), (['timing', 'of', 'college'], 'hours'), (['what', 'is', 'college', 'timing'], 'hours'), (['working', 'days'], 'hours'), (['when', 'are', 'you', 'guys', 'open'], 'hours'), (['what', 'are', 'your', 'hours'], 'hours'), (['hours', 'of', 'operation'], 'hours'), (['when', 'is', 'the', 'college', 'open'], 'hours'), (['college', 'timing'], 'hours'), (['what', 'about', 'college', 'timing'], 'hours'), (['is', 'college', 'open', 'on', 'saturday'], 'hours'), (['tell', 'something', 'about', 'college', 'timing'], 'hours'), (['what', 'is', 'the', 'college', 'hours'], 'hours'), (['when', 'should', 'i', 'come', 'to', 'college'], 'hours'), (['when', 'should', 'i', 'attend', 'college'], 'hours'), (['what', 'is', 'my', 'college', 'time'], 'hours'), (['college', 'timing'], 'hours'), (['timing', 'college'], 'hours'), (['more', 'info'], 'number'), (['contact', 'info'], 'number'), (['how', 'to', 'contact', 'college'], 'number'), (['college', 'telephone', 'number'], 'number'), (['college', 'number'], 'number'), (['What', 'is', 'your', 'contact', 'no'], 'number'), (['Contact', 'number', '?'], 'number'), (['how', 'to', 'call', 'you'], 'number'), (['College', 'phone', 'no', '?'], 'number'), (['how', 'can', 'i', 'contact', 'you'], 'number'), (['Can', 'i', 'get', 'your', 'phone', 'number'], 'number'), (['how', 'can', 'i', 'call', 'you'], 'number'), (['phone', 'number'], 'number'), (['phone', 'no'], 'number'), (['call'], 'number'), (['list', 'of', 'courses'], 'course'), (['list', 'of', 'courses', 'offered'], 'course'), (['list', 'of', 'courses', 'offered', 'in'], 'course'), (['what', 'are', 'the', 'courses', 'offered', 'in', 'your', 'college', '?'], 'course'), (['courses', '?'], 'course'), (['courses', 'offered'], 'course'), (['courses', 'offered', 'in', '(', 'your', 'univrsity', '(', 'UNI', ')', 'name', ')'], 'course'), (['courses', 'you', 'offer'], 'course'), (['branches', '?'], 'course'), (['courses', 'available', 'at', 'UNI', '?'], 'course'), (['branches', 'available', 'at', 'your', 'college', '?'], 'course'), (['what', 'are', 'the', 'courses', 'in', 'UNI', '?'], 'course'), (['what', 'are', 'branches', 'in', 'UNI', '?'], 'course'), (['what', 'are', 'courses', 'in', 'UNI', '?'], 'course'), (['branches', 'available', 'in', 'UNI', '?'], 'course'), (['can', 'you', 'tell', 'me', 'the', 'courses', 'available', 'in', 'UNI', '?'], 'course'), (['can', 'you', 'tell', 'me', 'the', 'branches', 'available', 'in', 'UNI', '?'], 'course'), (['computer', 'engineering', '?'], 'course'), (['computer'], 'course'), (['Computer', 'engineering', '?'], 'course'), (['it'], 'course'), (['IT'], 'course'), (['Information', 'Technology'], 'course'), (['AI/Ml'], 'course'), (['Mechanical', 'engineering'], 'course'), (['Chemical', 'engineering'], 'course'), (['Civil', 'engineering'], 'course'), (['information', 'about', 'fee'], 'fees'), (['information', 'on', 'fee'], 'fees'), (['tell', 'me', 'the', 'fee'], 'fees'), (['college', 'fee'], 'fees'), (['fee', 'per', 'semester'], 'fees'), (['what', 'is', 'the', 'fee', 'of', 'each', 'semester'], 'fees'), (['what', 'is', 'the', 'fees', 'of', 'each', 'year'], 'fees'), (['what', 'is', 'fee'], 'fees'), (['what', 'is', 'the', 'fees'], 'fees'), (['how', 'much', 'is', 'the', 'fees'], 'fees'), (['fees', 'for', 'first', 'year'], 'fees'), (['fees'], 'fees'), (['about', 'the', 'fees'], 'fees'), (['tell', 'me', 'something', 'about', 'the', 'fees'], 'fees'), (['What', 'is', 'the', 'fees', 'of', 'hostel'], 'fees'), (['how', 'much', 'is', 'the', 'fees'], 'fees'), (['hostel', 'fees'], 'fees'), (['fees', 'for', 'AC', 'room'], 'fees'), (['fees', 'for', 'non-AC', 'room'], 'fees'), (['fees', 'for', 'Ac', 'room', 'for', 'girls'], 'fees'), (['fees', 'for', 'non-Ac', 'room', 'for', 'girls'], 'fees'), (['fees', 'for', 'Ac', 'room', 'for', 'boys'], 'fees'), (['fees', 'for', 'non-Ac', 'room', 'for', 'boys'], 'fees'), (['where', 'is', 'the', 'college', 'located'], 'location'), (['college', 'is', 'located', 'at'], 'location'), (['where', 'is', 'college'], 'location'), (['where', 'is', 'college', 'located'], 'location'), (['address', 'of', 'college'], 'location'), (['how', 'to', 'reach', 'college'], 'location'), (['college', 'location'], 'location'), (['college', 'address'], 'location'), (['wheres', 'the', 'college'], 'location'), (['how', 'can', 'I', 'reach', 'college'], 'location'), (['whats', 'is', 'the', 'college', 'address'], 'location'), (['what', 'is', 'the', 'address', 'of', 'college'], 'location'), (['address'], 'location'), (['location'], 'location'), (['hostel', 'facility'], 'hostel'), (['hostel', 'servive'], 'hostel'), (['hostel', 'location'], 'hostel'), (['hostel', 'address'], 'hostel'), (['hostel', 'facilities'], 'hostel'), (['hostel', 'fees'], 'hostel'), (['Does', 'college', 'provide', 'hostel'], 'hostel'), (['Is', 'there', 'any', 'hostel'], 'hostel'), (['Where', 'is', 'hostel'], 'hostel'), (['do', 'you', 'have', 'hostel'], 'hostel'), (['do', 'you', 'guys', 'have', 'hostel'], 'hostel'), (['hostel'], 'hostel'), (['hostel', 'capacity'], 'hostel'), (['what', 'is', 'the', 'hostel', 'fee'], 'hostel'), (['how', 'to', 'get', 'in', 'hostel'], 'hostel'), (['what', 'is', 'the', 'hostel', 'address'], 'hostel'), (['how', 'far', 'is', 'hostel', 'from', 'college'], 'hostel'), (['hostel', 'college', 'distance'], 'hostel'), (['where', 'is', 'the', 'hostel'], 'hostel'), (['how', 'big', 'is', 'the', 'hostel'], 'hostel'), (['distance', 'between', 'college', 'and', 'hostel'], 'hostel'), (['distance', 'between', 'hostel', 'and', 'college'], 'hostel'), (['events', 'organised'], 'event'), (['list', 'of', 'events'], 'event'), (['list', 'of', 'events', 'organised', 'in', 'college'], 'event'), (['list', 'of', 'events', 'conducted', 'in', 'college'], 'event'), (['What', 'events', 'are', 'conducted', 'in', 'college'], 'event'), (['Are', 'there', 'any', 'event', 'held', 'at', 'college'], 'event'), (['Events', '?'], 'event'), (['functions'], 'event'), (['what', 'are', 'the', 'events'], 'event'), (['tell', 'me', 'about', 'events'], 'event'), (['what', 'about', 'events'], 'event'), (['document', 'to', 'bring'], 'document'), (['documents', 'needed', 'for', 'admision'], 'document'), (['documents', 'needed', 'at', 'the', 'time', 'of', 'admission'], 'document'), (['documents', 'needed', 'during', 'admission'], 'document'), (['documents', 'required', 'for', 'admision'], 'document'), (['documents', 'required', 'at', 'the', 'time', 'of', 'admission'], 'document'), (['documents', 'required', 'during', 'admission'], 'document'), (['What', 'document', 'are', 'required', 'for', 'admission'], 'document'), (['Which', 'document', 'to', 'bring', 'for', 'admission'], 'document'), (['documents'], 'document'), (['what', 'documents', 'do', 'i', 'need'], 'document'), (['what', 'documents', 'do', 'I', 'need', 'for', 'admission'], 'document'), (['documents', 'needed'], 'document'), (['size', 'of', 'campus'], 'floors'), (['building', 'size'], 'floors'), (['How', 'many', 'floors', 'does', 'college', 'have'], 'floors'), (['floors', 'in', 'college'], 'floors'), (['floors', 'in', 'college'], 'floors'), (['how', 'tall', 'is', 'UNI', \"'s\", 'College', 'of', 'Engineering', 'college', 'building'], 'floors'), (['floors'], 'floors'), (['Syllabus', 'for', 'IT'], 'syllabus'), (['what', 'is', 'the', 'Information', 'Technology', 'syllabus'], 'syllabus'), (['syllabus'], 'syllabus'), (['timetable'], 'syllabus'), (['what', 'is', 'IT', 'syllabus'], 'syllabus'), (['syllabus'], 'syllabus'), (['What', 'is', 'next', 'lecture'], 'syllabus'), (['is', 'there', 'any', 'library'], 'library'), (['library', 'facility'], 'library'), (['library', 'facilities'], 'library'), (['do', 'you', 'have', 'library'], 'library'), (['does', 'the', 'college', 'have', 'library', 'facility'], 'library'), (['college', 'library'], 'library'), (['where', 'can', 'i', 'get', 'books'], 'library'), (['book', 'facility'], 'library'), (['Where', 'is', 'library'], 'library'), (['Library'], 'library'), (['Library', 'information'], 'library'), (['Library', 'books', 'information'], 'library'), (['Tell', 'me', 'about', 'library'], 'library'), (['how', 'many', 'libraries'], 'library'), (['how', 'is', 'college', 'infrastructure'], 'infrastructure'), (['infrastructure'], 'infrastructure'), (['college', 'infrastructure'], 'infrastructure'), (['food', 'facilities'], 'canteen'), (['canteen', 'facilities'], 'canteen'), (['canteen', 'facility'], 'canteen'), (['is', 'there', 'any', 'canteen'], 'canteen'), (['Is', 'there', 'a', 'cafetaria', 'in', 'college'], 'canteen'), (['Does', 'college', 'have', 'canteen'], 'canteen'), (['Where', 'is', 'canteen'], 'canteen'), (['where', 'is', 'cafetaria'], 'canteen'), (['canteen'], 'canteen'), (['Food'], 'canteen'), (['Cafetaria'], 'canteen'), (['food', 'menu'], 'menu'), (['food', 'in', 'canteen'], 'menu'), (['Whats', 'there', 'on', 'menu'], 'menu'), (['what', 'is', 'available', 'in', 'college', 'canteen'], 'menu'), (['what', 'foods', 'can', 'we', 'get', 'in', 'college', 'canteen'], 'menu'), (['food', 'variety'], 'menu'), (['What', 'is', 'there', 'to', 'eat', '?'], 'menu'), (['What', 'is', 'college', 'placement'], 'placement'), (['Which', 'companies', 'visit', 'in', 'college'], 'placement'), (['What', 'is', 'average', 'package'], 'placement'), (['companies', 'visit'], 'placement'), (['package'], 'placement'), (['About', 'placement'], 'placement'), (['placement'], 'placement'), (['recruitment'], 'placement'), (['companies'], 'placement'), (['Who', 'is', 'HOD'], 'ithod'), (['Where', 'is', 'HOD'], 'ithod'), (['it', 'hod'], 'ithod'), (['name', 'of', 'it', 'hod'], 'ithod'), (['Who', 'is', 'computer', 'HOD'], 'computerhod'), (['Where', 'is', 'computer', 'HOD'], 'computerhod'), (['computer', 'hod'], 'computerhod'), (['name', 'of', 'computer', 'hod'], 'computerhod'), (['Who', 'is', 'extc', 'HOD'], 'extchod'), (['Where', 'is', 'extc', 'HOD'], 'extchod'), (['extc', 'hod'], 'extchod'), (['name', 'of', 'extc', 'hod'], 'extchod'), (['what', 'is', 'the', 'name', 'of', 'principal'], 'principal'), (['whatv', 'is', 'the', 'principal', 'name'], 'principal'), (['principal', 'name'], 'principal'), (['Who', 'is', 'college', 'principal'], 'principal'), (['Where', 'is', 'principal', \"'s\", 'office'], 'principal'), (['principal'], 'principal'), (['name', 'of', 'principal'], 'principal'), (['exam', 'dates'], 'sem'), (['exam', 'schedule'], 'sem'), (['When', 'is', 'semester', 'exam'], 'sem'), (['Semester', 'exam', 'timetable'], 'sem'), (['sem'], 'sem'), (['semester'], 'sem'), (['exam'], 'sem'), (['when', 'is', 'exam'], 'sem'), (['exam', 'timetable'], 'sem'), (['exam', 'dates'], 'sem'), (['when', 'is', 'semester'], 'sem'), (['what', 'is', 'the', 'process', 'of', 'admission'], 'admission'), (['what', 'is', 'the', 'admission', 'process'], 'admission'), (['How', 'to', 'take', 'admission', 'in', 'your', 'college'], 'admission'), (['What', 'is', 'the', 'process', 'for', 'admission'], 'admission'), (['admission'], 'admission'), (['admission', 'process'], 'admission'), (['scholarship'], 'scholarship'), (['Is', 'scholarship', 'available'], 'scholarship'), (['scholarship', 'engineering'], 'scholarship'), (['scholarship', 'it'], 'scholarship'), (['scholarship', 'ce'], 'scholarship'), (['scholarship', 'mechanical'], 'scholarship'), (['scholarship', 'civil'], 'scholarship'), (['scholarship', 'chemical'], 'scholarship'), (['scholarship', 'for', 'AI/ML'], 'scholarship'), (['available', 'scholarships'], 'scholarship'), (['scholarship', 'for', 'computer', 'engineering'], 'scholarship'), (['scholarship', 'for', 'IT', 'engineering'], 'scholarship'), (['scholarship', 'for', 'mechanical', 'engineering'], 'scholarship'), (['scholarship', 'for', 'civil', 'engineering'], 'scholarship'), (['scholarship', 'for', 'chemical', 'engineering'], 'scholarship'), (['list', 'of', 'scholarship'], 'scholarship'), (['comps', 'scholarship'], 'scholarship'), (['IT', 'scholarship'], 'scholarship'), (['mechanical', 'scholarship'], 'scholarship'), (['civil', 'scholarship'], 'scholarship'), (['chemical', 'scholarship'], 'scholarship'), (['automobile', 'scholarship'], 'scholarship'), (['first', 'year', 'scholarship'], 'scholarship'), (['second', 'year', 'scholarship'], 'scholarship'), (['third', 'year', 'scholarship'], 'scholarship'), (['fourth', 'year', 'scholarship'], 'scholarship'), (['What', 'facilities', 'college', 'provide'], 'facilities'), (['College', 'facility'], 'facilities'), (['What', 'are', 'college', 'facilities'], 'facilities'), (['facilities'], 'facilities'), (['facilities', 'provided'], 'facilities'), (['max', 'number', 'of', 'students'], 'college intake'), (['number', 'of', 'seats', 'per', 'branch'], 'college intake'), (['number', 'of', 'seats', 'in', 'each', 'branch'], 'college intake'), (['maximum', 'number', 'of', 'seats'], 'college intake'), (['maximum', 'students', 'intake'], 'college intake'), (['What', 'is', 'college', 'intake'], 'college intake'), (['how', 'many', 'stundent', 'are', 'taken', 'in', 'each', 'branch'], 'college intake'), (['seat', 'allotment'], 'college intake'), (['seats'], 'college intake'), (['college', 'dress', 'code'], 'uniform'), (['college', 'dresscode'], 'uniform'), (['what', 'is', 'the', 'uniform'], 'uniform'), (['can', 'we', 'wear', 'casuals'], 'uniform'), (['Does', 'college', 'have', 'an', 'uniform'], 'uniform'), (['Is', 'there', 'any', 'uniform'], 'uniform'), (['uniform'], 'uniform'), (['what', 'about', 'uniform'], 'uniform'), (['do', 'we', 'have', 'to', 'wear', 'uniform'], 'uniform'), (['what', 'are', 'the', 'different', 'committe', 'in', 'college'], 'committee'), (['different', 'committee', 'in', 'college'], 'committee'), (['Are', 'there', 'any', 'committee', 'in', 'college'], 'committee'), (['Give', 'me', 'committee', 'details'], 'committee'), (['committee'], 'committee'), (['how', 'many', 'committee', 'are', 'there', 'in', 'college'], 'committee'), (['I', 'love', 'you'], 'random'), (['Will', 'you', 'marry', 'me'], 'random'), (['Do', 'you', 'love', 'me'], 'random'), (['fuck'], 'swear'), (['bitch'], 'swear'), (['shut', 'up'], 'swear'), (['hell'], 'swear'), (['stupid'], 'swear'), (['idiot'], 'swear'), (['dumb', 'ass'], 'swear'), (['asshole'], 'swear'), (['fucker'], 'swear'), (['holidays'], 'vacation'), (['when', 'will', 'semester', 'starts'], 'vacation'), (['when', 'will', 'semester', 'end'], 'vacation'), (['when', 'is', 'the', 'holidays'], 'vacation'), (['list', 'of', 'holidays'], 'vacation'), (['Holiday', 'in', 'these', 'year'], 'vacation'), (['holiday', 'list'], 'vacation'), (['about', 'vacations'], 'vacation'), (['about', 'holidays'], 'vacation'), (['When', 'is', 'vacation'], 'vacation'), (['When', 'is', 'holidays'], 'vacation'), (['how', 'long', 'will', 'be', 'the', 'vacation'], 'vacation'), (['sports', 'and', 'games'], 'sports'), (['give', 'sports', 'details'], 'sports'), (['sports', 'infrastructure'], 'sports'), (['sports', 'facilities'], 'sports'), (['information', 'about', 'sports'], 'sports'), (['Sports', 'activities'], 'sports'), (['please', 'provide', 'sports', 'and', 'games', 'information'], 'sports'), (['okk'], 'salutaion'), (['okie'], 'salutaion'), (['nice', 'work'], 'salutaion'), (['well', 'done'], 'salutaion'), (['good', 'job'], 'salutaion'), (['thanks', 'for', 'the', 'help'], 'salutaion'), (['Thank', 'You'], 'salutaion'), (['its', 'ok'], 'salutaion'), (['Thanks'], 'salutaion'), (['Good', 'work'], 'salutaion'), (['k'], 'salutaion'), (['ok'], 'salutaion'), (['okay'], 'salutaion'), (['what', 'can', 'you', 'do'], 'task'), (['what', 'are', 'the', 'thing', 'you', 'can', 'do'], 'task'), (['things', 'you', 'can', 'do'], 'task'), (['what', 'can', 'u', 'do', 'for', 'me'], 'task'), (['how', 'u', 'can', 'help', 'me'], 'task'), (['why', 'i', 'should', 'use', 'you'], 'task'), (['ragging'], 'ragging'), (['is', 'ragging', 'practice', 'active', 'in', 'college'], 'ragging'), (['does', 'college', 'have', 'any', 'antiragging', 'facility'], 'ragging'), (['is', 'there', 'any', 'ragging', 'cases'], 'ragging'), (['is', 'ragging', 'done', 'here'], 'ragging'), (['ragging', 'against'], 'ragging'), (['antiragging', 'facility'], 'ragging'), (['ragging', 'juniors'], 'ragging'), (['ragging', 'history'], 'ragging'), (['ragging', 'incidents'], 'ragging'), (['hod'], 'hod'), (['hod', 'name'], 'hod'), (['who', 'is', 'the', 'hod'], 'hod')]\n"
          ]
        }
      ],
      "source": [
        "lemma = WordNetLemmatizer()  # Initialize the lemmatizer\n",
        "intents = json.loads(open('intents.json').read())  # Load the intents from the JSON file\n",
        "vocab = []  # List to store individual words\n",
        "category = []  # List to store intent classes\n",
        "dictonary = []  # List to store documents (word lists and intent tags)\n",
        "useless = ['?', '.', '!', ',']  # List of ignored characters\n",
        "\n",
        "# Iterate over each intent in the intents list\n",
        "for intent in intents['intents']:\n",
        "    # Iterate over each pattern in the patterns list of the current intent\n",
        "    for patterns in intent['patterns']:\n",
        "        word_list = nltk.word_tokenize(patterns)  # Tokenize the pattern into individual words\n",
        "        vocab.extend(word_list)  # Extend the words list with the individual words\n",
        "        dictonary.append((word_list, intent['tag']))  # Append the document as a tuple of word list and intent tag\n",
        "        if intent['tag'] not in category:\n",
        "            category.append(intent['tag'])  # Add the intent tag to the classes list if it doesn't already exist\n",
        "\n",
        "print(dictonary)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbz4kWuhxGZS",
        "outputId": "b1c9e33a-7b93-4328-e734-8125fd048e36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['admission',\n",
              " 'canteen',\n",
              " 'college intake',\n",
              " 'committee',\n",
              " 'computerhod',\n",
              " 'course',\n",
              " 'creator',\n",
              " 'document',\n",
              " 'event',\n",
              " 'extchod',\n",
              " 'facilities',\n",
              " 'fees',\n",
              " 'floors',\n",
              " 'goodbye',\n",
              " 'greeting',\n",
              " 'hod',\n",
              " 'hostel',\n",
              " 'hours',\n",
              " 'infrastructure',\n",
              " 'ithod',\n",
              " 'library',\n",
              " 'location',\n",
              " 'menu',\n",
              " 'name',\n",
              " 'number',\n",
              " 'placement',\n",
              " 'principal',\n",
              " 'ragging',\n",
              " 'random',\n",
              " 'salutaion',\n",
              " 'scholarship',\n",
              " 'sem',\n",
              " 'sports',\n",
              " 'swear',\n",
              " 'syllabus',\n",
              " 'task',\n",
              " 'uniform',\n",
              " 'vacation']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab=[lemmetizer.lemmatize(word) for word in vocab if word not in useless]\n",
        "#eliminate duplicates\n",
        "vocab=sorted(set(vocab))\n",
        "vocab\n",
        "category=sorted(set(category))\n",
        "category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4rSegvUxHK5"
      },
      "outputs": [],
      "source": [
        "pickle.dump(vocab, open('words.pkl', 'wb'))\n",
        "pickle.dump(category, open('classes.pkl', 'wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDlAuLU9xYJo",
        "outputId": "677f4501-745f-4bcc-d51b-0ea6b656bb9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hi']\n",
            "['hi'] *\n",
            "['How', 'are', 'you', '?']\n",
            "['how', 'are', 'you', '?'] *\n",
            "['Is', 'anyone', 'there', '?']\n",
            "['is', 'anyone', 'there', '?'] *\n",
            "['Hello']\n",
            "['hello'] *\n",
            "['Good', 'day']\n",
            "['good', 'day'] *\n",
            "['What', \"'s\", 'up']\n",
            "['what', \"'s\", 'up'] *\n",
            "['how', 'are', 'ya']\n",
            "['how', 'are', 'ya'] *\n",
            "['heyy']\n",
            "['heyy'] *\n",
            "['whatsup']\n",
            "['whatsup'] *\n",
            "['?', '?', '?', '?', '?', '?', '?', '?']\n",
            "['?', '?', '?', '?', '?', '?', '?', '?'] *\n",
            "['cya']\n",
            "['cya'] *\n",
            "['see', 'you']\n",
            "['see', 'you'] *\n",
            "['bye', 'bye']\n",
            "['bye', 'bye'] *\n",
            "['See', 'you', 'later']\n",
            "['see', 'you', 'later'] *\n",
            "['Goodbye']\n",
            "['goodbye'] *\n",
            "['I', 'am', 'Leaving']\n",
            "['i', 'am', 'leaving'] *\n",
            "['Bye']\n",
            "['bye'] *\n",
            "['Have', 'a', 'Good', 'day']\n",
            "['have', 'a', 'good', 'day'] *\n",
            "['talk', 'to', 'you', 'later']\n",
            "['talk', 'to', 'you', 'later'] *\n",
            "['ttyl']\n",
            "['ttyl'] *\n",
            "['i', 'got', 'to', 'go']\n",
            "['i', 'got', 'to', 'go'] *\n",
            "['gtg']\n",
            "['gtg'] *\n",
            "['what', 'is', 'the', 'name', 'of', 'your', 'developers']\n",
            "['what', 'is', 'the', 'name', 'of', 'your', 'developer'] *\n",
            "['what', 'is', 'the', 'name', 'of', 'your', 'creators']\n",
            "['what', 'is', 'the', 'name', 'of', 'your', 'creator'] *\n",
            "['what', 'is', 'the', 'name', 'of', 'the', 'developers']\n",
            "['what', 'is', 'the', 'name', 'of', 'the', 'developer'] *\n",
            "['what', 'is', 'the', 'name', 'of', 'the', 'creators']\n",
            "['what', 'is', 'the', 'name', 'of', 'the', 'creator'] *\n",
            "['who', 'created', 'you']\n",
            "['who', 'created', 'you'] *\n",
            "['your', 'developers']\n",
            "['your', 'developer'] *\n",
            "['your', 'creators']\n",
            "['your', 'creator'] *\n",
            "['who', 'are', 'your', 'developers']\n",
            "['who', 'are', 'your', 'developer'] *\n",
            "['developers']\n",
            "['developer'] *\n",
            "['you', 'are', 'made', 'by']\n",
            "['you', 'are', 'made', 'by'] *\n",
            "['you', 'are', 'made', 'by', 'whom']\n",
            "['you', 'are', 'made', 'by', 'whom'] *\n",
            "['who', 'created', 'you']\n",
            "['who', 'created', 'you'] *\n",
            "['who', 'create', 'you']\n",
            "['who', 'create', 'you'] *\n",
            "['creators']\n",
            "['creator'] *\n",
            "['who', 'made', 'you']\n",
            "['who', 'made', 'you'] *\n",
            "['who', 'designed', 'you']\n",
            "['who', 'designed', 'you'] *\n",
            "['name']\n",
            "['name'] *\n",
            "['your', 'name']\n",
            "['your', 'name'] *\n",
            "['do', 'you', 'have', 'a', 'name']\n",
            "['do', 'you', 'have', 'a', 'name'] *\n",
            "['what', 'are', 'you', 'called']\n",
            "['what', 'are', 'you', 'called'] *\n",
            "['what', 'is', 'your', 'name']\n",
            "['what', 'is', 'your', 'name'] *\n",
            "['what', 'should', 'I', 'call', 'you']\n",
            "['what', 'should', 'i', 'call', 'you'] *\n",
            "['whats', 'your', 'name', '?']\n",
            "['whats', 'your', 'name', '?'] *\n",
            "['what', 'are', 'you']\n",
            "['what', 'are', 'you'] *\n",
            "['who', 'are', 'you']\n",
            "['who', 'are', 'you'] *\n",
            "['who', 'is', 'this']\n",
            "['who', 'is', 'this'] *\n",
            "['what', 'am', 'i', 'chatting', 'to']\n",
            "['what', 'am', 'i', 'chatting', 'to'] *\n",
            "['who', 'am', 'i', 'taking', 'to']\n",
            "['who', 'am', 'i', 'taking', 'to'] *\n",
            "['what', 'are', 'you']\n",
            "['what', 'are', 'you'] *\n",
            "['timing', 'of', 'college']\n",
            "['timing', 'of', 'college'] *\n",
            "['what', 'is', 'college', 'timing']\n",
            "['what', 'is', 'college', 'timing'] *\n",
            "['working', 'days']\n",
            "['working', 'day'] *\n",
            "['when', 'are', 'you', 'guys', 'open']\n",
            "['when', 'are', 'you', 'guy', 'open'] *\n",
            "['what', 'are', 'your', 'hours']\n",
            "['what', 'are', 'your', 'hour'] *\n",
            "['hours', 'of', 'operation']\n",
            "['hour', 'of', 'operation'] *\n",
            "['when', 'is', 'the', 'college', 'open']\n",
            "['when', 'is', 'the', 'college', 'open'] *\n",
            "['college', 'timing']\n",
            "['college', 'timing'] *\n",
            "['what', 'about', 'college', 'timing']\n",
            "['what', 'about', 'college', 'timing'] *\n",
            "['is', 'college', 'open', 'on', 'saturday']\n",
            "['is', 'college', 'open', 'on', 'saturday'] *\n",
            "['tell', 'something', 'about', 'college', 'timing']\n",
            "['tell', 'something', 'about', 'college', 'timing'] *\n",
            "['what', 'is', 'the', 'college', 'hours']\n",
            "['what', 'is', 'the', 'college', 'hour'] *\n",
            "['when', 'should', 'i', 'come', 'to', 'college']\n",
            "['when', 'should', 'i', 'come', 'to', 'college'] *\n",
            "['when', 'should', 'i', 'attend', 'college']\n",
            "['when', 'should', 'i', 'attend', 'college'] *\n",
            "['what', 'is', 'my', 'college', 'time']\n",
            "['what', 'is', 'my', 'college', 'time'] *\n",
            "['college', 'timing']\n",
            "['college', 'timing'] *\n",
            "['timing', 'college']\n",
            "['timing', 'college'] *\n",
            "['more', 'info']\n",
            "['more', 'info'] *\n",
            "['contact', 'info']\n",
            "['contact', 'info'] *\n",
            "['how', 'to', 'contact', 'college']\n",
            "['how', 'to', 'contact', 'college'] *\n",
            "['college', 'telephone', 'number']\n",
            "['college', 'telephone', 'number'] *\n",
            "['college', 'number']\n",
            "['college', 'number'] *\n",
            "['What', 'is', 'your', 'contact', 'no']\n",
            "['what', 'is', 'your', 'contact', 'no'] *\n",
            "['Contact', 'number', '?']\n",
            "['contact', 'number', '?'] *\n",
            "['how', 'to', 'call', 'you']\n",
            "['how', 'to', 'call', 'you'] *\n",
            "['College', 'phone', 'no', '?']\n",
            "['college', 'phone', 'no', '?'] *\n",
            "['how', 'can', 'i', 'contact', 'you']\n",
            "['how', 'can', 'i', 'contact', 'you'] *\n",
            "['Can', 'i', 'get', 'your', 'phone', 'number']\n",
            "['can', 'i', 'get', 'your', 'phone', 'number'] *\n",
            "['how', 'can', 'i', 'call', 'you']\n",
            "['how', 'can', 'i', 'call', 'you'] *\n",
            "['phone', 'number']\n",
            "['phone', 'number'] *\n",
            "['phone', 'no']\n",
            "['phone', 'no'] *\n",
            "['call']\n",
            "['call'] *\n",
            "['list', 'of', 'courses']\n",
            "['list', 'of', 'course'] *\n",
            "['list', 'of', 'courses', 'offered']\n",
            "['list', 'of', 'course', 'offered'] *\n",
            "['list', 'of', 'courses', 'offered', 'in']\n",
            "['list', 'of', 'course', 'offered', 'in'] *\n",
            "['what', 'are', 'the', 'courses', 'offered', 'in', 'your', 'college', '?']\n",
            "['what', 'are', 'the', 'course', 'offered', 'in', 'your', 'college', '?'] *\n",
            "['courses', '?']\n",
            "['course', '?'] *\n",
            "['courses', 'offered']\n",
            "['course', 'offered'] *\n",
            "['courses', 'offered', 'in', '(', 'your', 'univrsity', '(', 'UNI', ')', 'name', ')']\n",
            "['course', 'offered', 'in', '(', 'your', 'univrsity', '(', 'uni', ')', 'name', ')'] *\n",
            "['courses', 'you', 'offer']\n",
            "['course', 'you', 'offer'] *\n",
            "['branches', '?']\n",
            "['branch', '?'] *\n",
            "['courses', 'available', 'at', 'UNI', '?']\n",
            "['course', 'available', 'at', 'uni', '?'] *\n",
            "['branches', 'available', 'at', 'your', 'college', '?']\n",
            "['branch', 'available', 'at', 'your', 'college', '?'] *\n",
            "['what', 'are', 'the', 'courses', 'in', 'UNI', '?']\n",
            "['what', 'are', 'the', 'course', 'in', 'uni', '?'] *\n",
            "['what', 'are', 'branches', 'in', 'UNI', '?']\n",
            "['what', 'are', 'branch', 'in', 'uni', '?'] *\n",
            "['what', 'are', 'courses', 'in', 'UNI', '?']\n",
            "['what', 'are', 'course', 'in', 'uni', '?'] *\n",
            "['branches', 'available', 'in', 'UNI', '?']\n",
            "['branch', 'available', 'in', 'uni', '?'] *\n",
            "['can', 'you', 'tell', 'me', 'the', 'courses', 'available', 'in', 'UNI', '?']\n",
            "['can', 'you', 'tell', 'me', 'the', 'course', 'available', 'in', 'uni', '?'] *\n",
            "['can', 'you', 'tell', 'me', 'the', 'branches', 'available', 'in', 'UNI', '?']\n",
            "['can', 'you', 'tell', 'me', 'the', 'branch', 'available', 'in', 'uni', '?'] *\n",
            "['computer', 'engineering', '?']\n",
            "['computer', 'engineering', '?'] *\n",
            "['computer']\n",
            "['computer'] *\n",
            "['Computer', 'engineering', '?']\n",
            "['computer', 'engineering', '?'] *\n",
            "['it']\n",
            "['it'] *\n",
            "['IT']\n",
            "['it'] *\n",
            "['Information', 'Technology']\n",
            "['information', 'technology'] *\n",
            "['AI/Ml']\n",
            "['ai/ml'] *\n",
            "['Mechanical', 'engineering']\n",
            "['mechanical', 'engineering'] *\n",
            "['Chemical', 'engineering']\n",
            "['chemical', 'engineering'] *\n",
            "['Civil', 'engineering']\n",
            "['civil', 'engineering'] *\n",
            "['information', 'about', 'fee']\n",
            "['information', 'about', 'fee'] *\n",
            "['information', 'on', 'fee']\n",
            "['information', 'on', 'fee'] *\n",
            "['tell', 'me', 'the', 'fee']\n",
            "['tell', 'me', 'the', 'fee'] *\n",
            "['college', 'fee']\n",
            "['college', 'fee'] *\n",
            "['fee', 'per', 'semester']\n",
            "['fee', 'per', 'semester'] *\n",
            "['what', 'is', 'the', 'fee', 'of', 'each', 'semester']\n",
            "['what', 'is', 'the', 'fee', 'of', 'each', 'semester'] *\n",
            "['what', 'is', 'the', 'fees', 'of', 'each', 'year']\n",
            "['what', 'is', 'the', 'fee', 'of', 'each', 'year'] *\n",
            "['what', 'is', 'fee']\n",
            "['what', 'is', 'fee'] *\n",
            "['what', 'is', 'the', 'fees']\n",
            "['what', 'is', 'the', 'fee'] *\n",
            "['how', 'much', 'is', 'the', 'fees']\n",
            "['how', 'much', 'is', 'the', 'fee'] *\n",
            "['fees', 'for', 'first', 'year']\n",
            "['fee', 'for', 'first', 'year'] *\n",
            "['fees']\n",
            "['fee'] *\n",
            "['about', 'the', 'fees']\n",
            "['about', 'the', 'fee'] *\n",
            "['tell', 'me', 'something', 'about', 'the', 'fees']\n",
            "['tell', 'me', 'something', 'about', 'the', 'fee'] *\n",
            "['What', 'is', 'the', 'fees', 'of', 'hostel']\n",
            "['what', 'is', 'the', 'fee', 'of', 'hostel'] *\n",
            "['how', 'much', 'is', 'the', 'fees']\n",
            "['how', 'much', 'is', 'the', 'fee'] *\n",
            "['hostel', 'fees']\n",
            "['hostel', 'fee'] *\n",
            "['fees', 'for', 'AC', 'room']\n",
            "['fee', 'for', 'ac', 'room'] *\n",
            "['fees', 'for', 'non-AC', 'room']\n",
            "['fee', 'for', 'non-ac', 'room'] *\n",
            "['fees', 'for', 'Ac', 'room', 'for', 'girls']\n",
            "['fee', 'for', 'ac', 'room', 'for', 'girl'] *\n",
            "['fees', 'for', 'non-Ac', 'room', 'for', 'girls']\n",
            "['fee', 'for', 'non-ac', 'room', 'for', 'girl'] *\n",
            "['fees', 'for', 'Ac', 'room', 'for', 'boys']\n",
            "['fee', 'for', 'ac', 'room', 'for', 'boy'] *\n",
            "['fees', 'for', 'non-Ac', 'room', 'for', 'boys']\n",
            "['fee', 'for', 'non-ac', 'room', 'for', 'boy'] *\n",
            "['where', 'is', 'the', 'college', 'located']\n",
            "['where', 'is', 'the', 'college', 'located'] *\n",
            "['college', 'is', 'located', 'at']\n",
            "['college', 'is', 'located', 'at'] *\n",
            "['where', 'is', 'college']\n",
            "['where', 'is', 'college'] *\n",
            "['where', 'is', 'college', 'located']\n",
            "['where', 'is', 'college', 'located'] *\n",
            "['address', 'of', 'college']\n",
            "['address', 'of', 'college'] *\n",
            "['how', 'to', 'reach', 'college']\n",
            "['how', 'to', 'reach', 'college'] *\n",
            "['college', 'location']\n",
            "['college', 'location'] *\n",
            "['college', 'address']\n",
            "['college', 'address'] *\n",
            "['wheres', 'the', 'college']\n",
            "['wheres', 'the', 'college'] *\n",
            "['how', 'can', 'I', 'reach', 'college']\n",
            "['how', 'can', 'i', 'reach', 'college'] *\n",
            "['whats', 'is', 'the', 'college', 'address']\n",
            "['whats', 'is', 'the', 'college', 'address'] *\n",
            "['what', 'is', 'the', 'address', 'of', 'college']\n",
            "['what', 'is', 'the', 'address', 'of', 'college'] *\n",
            "['address']\n",
            "['address'] *\n",
            "['location']\n",
            "['location'] *\n",
            "['hostel', 'facility']\n",
            "['hostel', 'facility'] *\n",
            "['hostel', 'servive']\n",
            "['hostel', 'servive'] *\n",
            "['hostel', 'location']\n",
            "['hostel', 'location'] *\n",
            "['hostel', 'address']\n",
            "['hostel', 'address'] *\n",
            "['hostel', 'facilities']\n",
            "['hostel', 'facility'] *\n",
            "['hostel', 'fees']\n",
            "['hostel', 'fee'] *\n",
            "['Does', 'college', 'provide', 'hostel']\n",
            "['doe', 'college', 'provide', 'hostel'] *\n",
            "['Is', 'there', 'any', 'hostel']\n",
            "['is', 'there', 'any', 'hostel'] *\n",
            "['Where', 'is', 'hostel']\n",
            "['where', 'is', 'hostel'] *\n",
            "['do', 'you', 'have', 'hostel']\n",
            "['do', 'you', 'have', 'hostel'] *\n",
            "['do', 'you', 'guys', 'have', 'hostel']\n",
            "['do', 'you', 'guy', 'have', 'hostel'] *\n",
            "['hostel']\n",
            "['hostel'] *\n",
            "['hostel', 'capacity']\n",
            "['hostel', 'capacity'] *\n",
            "['what', 'is', 'the', 'hostel', 'fee']\n",
            "['what', 'is', 'the', 'hostel', 'fee'] *\n",
            "['how', 'to', 'get', 'in', 'hostel']\n",
            "['how', 'to', 'get', 'in', 'hostel'] *\n",
            "['what', 'is', 'the', 'hostel', 'address']\n",
            "['what', 'is', 'the', 'hostel', 'address'] *\n",
            "['how', 'far', 'is', 'hostel', 'from', 'college']\n",
            "['how', 'far', 'is', 'hostel', 'from', 'college'] *\n",
            "['hostel', 'college', 'distance']\n",
            "['hostel', 'college', 'distance'] *\n",
            "['where', 'is', 'the', 'hostel']\n",
            "['where', 'is', 'the', 'hostel'] *\n",
            "['how', 'big', 'is', 'the', 'hostel']\n",
            "['how', 'big', 'is', 'the', 'hostel'] *\n",
            "['distance', 'between', 'college', 'and', 'hostel']\n",
            "['distance', 'between', 'college', 'and', 'hostel'] *\n",
            "['distance', 'between', 'hostel', 'and', 'college']\n",
            "['distance', 'between', 'hostel', 'and', 'college'] *\n",
            "['events', 'organised']\n",
            "['event', 'organised'] *\n",
            "['list', 'of', 'events']\n",
            "['list', 'of', 'event'] *\n",
            "['list', 'of', 'events', 'organised', 'in', 'college']\n",
            "['list', 'of', 'event', 'organised', 'in', 'college'] *\n",
            "['list', 'of', 'events', 'conducted', 'in', 'college']\n",
            "['list', 'of', 'event', 'conducted', 'in', 'college'] *\n",
            "['What', 'events', 'are', 'conducted', 'in', 'college']\n",
            "['what', 'event', 'are', 'conducted', 'in', 'college'] *\n",
            "['Are', 'there', 'any', 'event', 'held', 'at', 'college']\n",
            "['are', 'there', 'any', 'event', 'held', 'at', 'college'] *\n",
            "['Events', '?']\n",
            "['event', '?'] *\n",
            "['functions']\n",
            "['function'] *\n",
            "['what', 'are', 'the', 'events']\n",
            "['what', 'are', 'the', 'event'] *\n",
            "['tell', 'me', 'about', 'events']\n",
            "['tell', 'me', 'about', 'event'] *\n",
            "['what', 'about', 'events']\n",
            "['what', 'about', 'event'] *\n",
            "['document', 'to', 'bring']\n",
            "['document', 'to', 'bring'] *\n",
            "['documents', 'needed', 'for', 'admision']\n",
            "['document', 'needed', 'for', 'admision'] *\n",
            "['documents', 'needed', 'at', 'the', 'time', 'of', 'admission']\n",
            "['document', 'needed', 'at', 'the', 'time', 'of', 'admission'] *\n",
            "['documents', 'needed', 'during', 'admission']\n",
            "['document', 'needed', 'during', 'admission'] *\n",
            "['documents', 'required', 'for', 'admision']\n",
            "['document', 'required', 'for', 'admision'] *\n",
            "['documents', 'required', 'at', 'the', 'time', 'of', 'admission']\n",
            "['document', 'required', 'at', 'the', 'time', 'of', 'admission'] *\n",
            "['documents', 'required', 'during', 'admission']\n",
            "['document', 'required', 'during', 'admission'] *\n",
            "['What', 'document', 'are', 'required', 'for', 'admission']\n",
            "['what', 'document', 'are', 'required', 'for', 'admission'] *\n",
            "['Which', 'document', 'to', 'bring', 'for', 'admission']\n",
            "['which', 'document', 'to', 'bring', 'for', 'admission'] *\n",
            "['documents']\n",
            "['document'] *\n",
            "['what', 'documents', 'do', 'i', 'need']\n",
            "['what', 'document', 'do', 'i', 'need'] *\n",
            "['what', 'documents', 'do', 'I', 'need', 'for', 'admission']\n",
            "['what', 'document', 'do', 'i', 'need', 'for', 'admission'] *\n",
            "['documents', 'needed']\n",
            "['document', 'needed'] *\n",
            "['size', 'of', 'campus']\n",
            "['size', 'of', 'campus'] *\n",
            "['building', 'size']\n",
            "['building', 'size'] *\n",
            "['How', 'many', 'floors', 'does', 'college', 'have']\n",
            "['how', 'many', 'floor', 'doe', 'college', 'have'] *\n",
            "['floors', 'in', 'college']\n",
            "['floor', 'in', 'college'] *\n",
            "['floors', 'in', 'college']\n",
            "['floor', 'in', 'college'] *\n",
            "['how', 'tall', 'is', 'UNI', \"'s\", 'College', 'of', 'Engineering', 'college', 'building']\n",
            "['how', 'tall', 'is', 'uni', \"'s\", 'college', 'of', 'engineering', 'college', 'building'] *\n",
            "['floors']\n",
            "['floor'] *\n",
            "['Syllabus', 'for', 'IT']\n",
            "['syllabus', 'for', 'it'] *\n",
            "['what', 'is', 'the', 'Information', 'Technology', 'syllabus']\n",
            "['what', 'is', 'the', 'information', 'technology', 'syllabus'] *\n",
            "['syllabus']\n",
            "['syllabus'] *\n",
            "['timetable']\n",
            "['timetable'] *\n",
            "['what', 'is', 'IT', 'syllabus']\n",
            "['what', 'is', 'it', 'syllabus'] *\n",
            "['syllabus']\n",
            "['syllabus'] *\n",
            "['What', 'is', 'next', 'lecture']\n",
            "['what', 'is', 'next', 'lecture'] *\n",
            "['is', 'there', 'any', 'library']\n",
            "['is', 'there', 'any', 'library'] *\n",
            "['library', 'facility']\n",
            "['library', 'facility'] *\n",
            "['library', 'facilities']\n",
            "['library', 'facility'] *\n",
            "['do', 'you', 'have', 'library']\n",
            "['do', 'you', 'have', 'library'] *\n",
            "['does', 'the', 'college', 'have', 'library', 'facility']\n",
            "['doe', 'the', 'college', 'have', 'library', 'facility'] *\n",
            "['college', 'library']\n",
            "['college', 'library'] *\n",
            "['where', 'can', 'i', 'get', 'books']\n",
            "['where', 'can', 'i', 'get', 'book'] *\n",
            "['book', 'facility']\n",
            "['book', 'facility'] *\n",
            "['Where', 'is', 'library']\n",
            "['where', 'is', 'library'] *\n",
            "['Library']\n",
            "['library'] *\n",
            "['Library', 'information']\n",
            "['library', 'information'] *\n",
            "['Library', 'books', 'information']\n",
            "['library', 'book', 'information'] *\n",
            "['Tell', 'me', 'about', 'library']\n",
            "['tell', 'me', 'about', 'library'] *\n",
            "['how', 'many', 'libraries']\n",
            "['how', 'many', 'library'] *\n",
            "['how', 'is', 'college', 'infrastructure']\n",
            "['how', 'is', 'college', 'infrastructure'] *\n",
            "['infrastructure']\n",
            "['infrastructure'] *\n",
            "['college', 'infrastructure']\n",
            "['college', 'infrastructure'] *\n",
            "['food', 'facilities']\n",
            "['food', 'facility'] *\n",
            "['canteen', 'facilities']\n",
            "['canteen', 'facility'] *\n",
            "['canteen', 'facility']\n",
            "['canteen', 'facility'] *\n",
            "['is', 'there', 'any', 'canteen']\n",
            "['is', 'there', 'any', 'canteen'] *\n",
            "['Is', 'there', 'a', 'cafetaria', 'in', 'college']\n",
            "['is', 'there', 'a', 'cafetaria', 'in', 'college'] *\n",
            "['Does', 'college', 'have', 'canteen']\n",
            "['doe', 'college', 'have', 'canteen'] *\n",
            "['Where', 'is', 'canteen']\n",
            "['where', 'is', 'canteen'] *\n",
            "['where', 'is', 'cafetaria']\n",
            "['where', 'is', 'cafetaria'] *\n",
            "['canteen']\n",
            "['canteen'] *\n",
            "['Food']\n",
            "['food'] *\n",
            "['Cafetaria']\n",
            "['cafetaria'] *\n",
            "['food', 'menu']\n",
            "['food', 'menu'] *\n",
            "['food', 'in', 'canteen']\n",
            "['food', 'in', 'canteen'] *\n",
            "['Whats', 'there', 'on', 'menu']\n",
            "['whats', 'there', 'on', 'menu'] *\n",
            "['what', 'is', 'available', 'in', 'college', 'canteen']\n",
            "['what', 'is', 'available', 'in', 'college', 'canteen'] *\n",
            "['what', 'foods', 'can', 'we', 'get', 'in', 'college', 'canteen']\n",
            "['what', 'food', 'can', 'we', 'get', 'in', 'college', 'canteen'] *\n",
            "['food', 'variety']\n",
            "['food', 'variety'] *\n",
            "['What', 'is', 'there', 'to', 'eat', '?']\n",
            "['what', 'is', 'there', 'to', 'eat', '?'] *\n",
            "['What', 'is', 'college', 'placement']\n",
            "['what', 'is', 'college', 'placement'] *\n",
            "['Which', 'companies', 'visit', 'in', 'college']\n",
            "['which', 'company', 'visit', 'in', 'college'] *\n",
            "['What', 'is', 'average', 'package']\n",
            "['what', 'is', 'average', 'package'] *\n",
            "['companies', 'visit']\n",
            "['company', 'visit'] *\n",
            "['package']\n",
            "['package'] *\n",
            "['About', 'placement']\n",
            "['about', 'placement'] *\n",
            "['placement']\n",
            "['placement'] *\n",
            "['recruitment']\n",
            "['recruitment'] *\n",
            "['companies']\n",
            "['company'] *\n",
            "['Who', 'is', 'HOD']\n",
            "['who', 'is', 'hod'] *\n",
            "['Where', 'is', 'HOD']\n",
            "['where', 'is', 'hod'] *\n",
            "['it', 'hod']\n",
            "['it', 'hod'] *\n",
            "['name', 'of', 'it', 'hod']\n",
            "['name', 'of', 'it', 'hod'] *\n",
            "['Who', 'is', 'computer', 'HOD']\n",
            "['who', 'is', 'computer', 'hod'] *\n",
            "['Where', 'is', 'computer', 'HOD']\n",
            "['where', 'is', 'computer', 'hod'] *\n",
            "['computer', 'hod']\n",
            "['computer', 'hod'] *\n",
            "['name', 'of', 'computer', 'hod']\n",
            "['name', 'of', 'computer', 'hod'] *\n",
            "['Who', 'is', 'extc', 'HOD']\n",
            "['who', 'is', 'extc', 'hod'] *\n",
            "['Where', 'is', 'extc', 'HOD']\n",
            "['where', 'is', 'extc', 'hod'] *\n",
            "['extc', 'hod']\n",
            "['extc', 'hod'] *\n",
            "['name', 'of', 'extc', 'hod']\n",
            "['name', 'of', 'extc', 'hod'] *\n",
            "['what', 'is', 'the', 'name', 'of', 'principal']\n",
            "['what', 'is', 'the', 'name', 'of', 'principal'] *\n",
            "['whatv', 'is', 'the', 'principal', 'name']\n",
            "['whatv', 'is', 'the', 'principal', 'name'] *\n",
            "['principal', 'name']\n",
            "['principal', 'name'] *\n",
            "['Who', 'is', 'college', 'principal']\n",
            "['who', 'is', 'college', 'principal'] *\n",
            "['Where', 'is', 'principal', \"'s\", 'office']\n",
            "['where', 'is', 'principal', \"'s\", 'office'] *\n",
            "['principal']\n",
            "['principal'] *\n",
            "['name', 'of', 'principal']\n",
            "['name', 'of', 'principal'] *\n",
            "['exam', 'dates']\n",
            "['exam', 'date'] *\n",
            "['exam', 'schedule']\n",
            "['exam', 'schedule'] *\n",
            "['When', 'is', 'semester', 'exam']\n",
            "['when', 'is', 'semester', 'exam'] *\n",
            "['Semester', 'exam', 'timetable']\n",
            "['semester', 'exam', 'timetable'] *\n",
            "['sem']\n",
            "['sem'] *\n",
            "['semester']\n",
            "['semester'] *\n",
            "['exam']\n",
            "['exam'] *\n",
            "['when', 'is', 'exam']\n",
            "['when', 'is', 'exam'] *\n",
            "['exam', 'timetable']\n",
            "['exam', 'timetable'] *\n",
            "['exam', 'dates']\n",
            "['exam', 'date'] *\n",
            "['when', 'is', 'semester']\n",
            "['when', 'is', 'semester'] *\n",
            "['what', 'is', 'the', 'process', 'of', 'admission']\n",
            "['what', 'is', 'the', 'process', 'of', 'admission'] *\n",
            "['what', 'is', 'the', 'admission', 'process']\n",
            "['what', 'is', 'the', 'admission', 'process'] *\n",
            "['How', 'to', 'take', 'admission', 'in', 'your', 'college']\n",
            "['how', 'to', 'take', 'admission', 'in', 'your', 'college'] *\n",
            "['What', 'is', 'the', 'process', 'for', 'admission']\n",
            "['what', 'is', 'the', 'process', 'for', 'admission'] *\n",
            "['admission']\n",
            "['admission'] *\n",
            "['admission', 'process']\n",
            "['admission', 'process'] *\n",
            "['scholarship']\n",
            "['scholarship'] *\n",
            "['Is', 'scholarship', 'available']\n",
            "['is', 'scholarship', 'available'] *\n",
            "['scholarship', 'engineering']\n",
            "['scholarship', 'engineering'] *\n",
            "['scholarship', 'it']\n",
            "['scholarship', 'it'] *\n",
            "['scholarship', 'ce']\n",
            "['scholarship', 'ce'] *\n",
            "['scholarship', 'mechanical']\n",
            "['scholarship', 'mechanical'] *\n",
            "['scholarship', 'civil']\n",
            "['scholarship', 'civil'] *\n",
            "['scholarship', 'chemical']\n",
            "['scholarship', 'chemical'] *\n",
            "['scholarship', 'for', 'AI/ML']\n",
            "['scholarship', 'for', 'ai/ml'] *\n",
            "['available', 'scholarships']\n",
            "['available', 'scholarship'] *\n",
            "['scholarship', 'for', 'computer', 'engineering']\n",
            "['scholarship', 'for', 'computer', 'engineering'] *\n",
            "['scholarship', 'for', 'IT', 'engineering']\n",
            "['scholarship', 'for', 'it', 'engineering'] *\n",
            "['scholarship', 'for', 'mechanical', 'engineering']\n",
            "['scholarship', 'for', 'mechanical', 'engineering'] *\n",
            "['scholarship', 'for', 'civil', 'engineering']\n",
            "['scholarship', 'for', 'civil', 'engineering'] *\n",
            "['scholarship', 'for', 'chemical', 'engineering']\n",
            "['scholarship', 'for', 'chemical', 'engineering'] *\n",
            "['list', 'of', 'scholarship']\n",
            "['list', 'of', 'scholarship'] *\n",
            "['comps', 'scholarship']\n",
            "['comp', 'scholarship'] *\n",
            "['IT', 'scholarship']\n",
            "['it', 'scholarship'] *\n",
            "['mechanical', 'scholarship']\n",
            "['mechanical', 'scholarship'] *\n",
            "['civil', 'scholarship']\n",
            "['civil', 'scholarship'] *\n",
            "['chemical', 'scholarship']\n",
            "['chemical', 'scholarship'] *\n",
            "['automobile', 'scholarship']\n",
            "['automobile', 'scholarship'] *\n",
            "['first', 'year', 'scholarship']\n",
            "['first', 'year', 'scholarship'] *\n",
            "['second', 'year', 'scholarship']\n",
            "['second', 'year', 'scholarship'] *\n",
            "['third', 'year', 'scholarship']\n",
            "['third', 'year', 'scholarship'] *\n",
            "['fourth', 'year', 'scholarship']\n",
            "['fourth', 'year', 'scholarship'] *\n",
            "['What', 'facilities', 'college', 'provide']\n",
            "['what', 'facility', 'college', 'provide'] *\n",
            "['College', 'facility']\n",
            "['college', 'facility'] *\n",
            "['What', 'are', 'college', 'facilities']\n",
            "['what', 'are', 'college', 'facility'] *\n",
            "['facilities']\n",
            "['facility'] *\n",
            "['facilities', 'provided']\n",
            "['facility', 'provided'] *\n",
            "['max', 'number', 'of', 'students']\n",
            "['max', 'number', 'of', 'student'] *\n",
            "['number', 'of', 'seats', 'per', 'branch']\n",
            "['number', 'of', 'seat', 'per', 'branch'] *\n",
            "['number', 'of', 'seats', 'in', 'each', 'branch']\n",
            "['number', 'of', 'seat', 'in', 'each', 'branch'] *\n",
            "['maximum', 'number', 'of', 'seats']\n",
            "['maximum', 'number', 'of', 'seat'] *\n",
            "['maximum', 'students', 'intake']\n",
            "['maximum', 'student', 'intake'] *\n",
            "['What', 'is', 'college', 'intake']\n",
            "['what', 'is', 'college', 'intake'] *\n",
            "['how', 'many', 'stundent', 'are', 'taken', 'in', 'each', 'branch']\n",
            "['how', 'many', 'stundent', 'are', 'taken', 'in', 'each', 'branch'] *\n",
            "['seat', 'allotment']\n",
            "['seat', 'allotment'] *\n",
            "['seats']\n",
            "['seat'] *\n",
            "['college', 'dress', 'code']\n",
            "['college', 'dress', 'code'] *\n",
            "['college', 'dresscode']\n",
            "['college', 'dresscode'] *\n",
            "['what', 'is', 'the', 'uniform']\n",
            "['what', 'is', 'the', 'uniform'] *\n",
            "['can', 'we', 'wear', 'casuals']\n",
            "['can', 'we', 'wear', 'casuals'] *\n",
            "['Does', 'college', 'have', 'an', 'uniform']\n",
            "['doe', 'college', 'have', 'an', 'uniform'] *\n",
            "['Is', 'there', 'any', 'uniform']\n",
            "['is', 'there', 'any', 'uniform'] *\n",
            "['uniform']\n",
            "['uniform'] *\n",
            "['what', 'about', 'uniform']\n",
            "['what', 'about', 'uniform'] *\n",
            "['do', 'we', 'have', 'to', 'wear', 'uniform']\n",
            "['do', 'we', 'have', 'to', 'wear', 'uniform'] *\n",
            "['what', 'are', 'the', 'different', 'committe', 'in', 'college']\n",
            "['what', 'are', 'the', 'different', 'committe', 'in', 'college'] *\n",
            "['different', 'committee', 'in', 'college']\n",
            "['different', 'committee', 'in', 'college'] *\n",
            "['Are', 'there', 'any', 'committee', 'in', 'college']\n",
            "['are', 'there', 'any', 'committee', 'in', 'college'] *\n",
            "['Give', 'me', 'committee', 'details']\n",
            "['give', 'me', 'committee', 'detail'] *\n",
            "['committee']\n",
            "['committee'] *\n",
            "['how', 'many', 'committee', 'are', 'there', 'in', 'college']\n",
            "['how', 'many', 'committee', 'are', 'there', 'in', 'college'] *\n",
            "['I', 'love', 'you']\n",
            "['i', 'love', 'you'] *\n",
            "['Will', 'you', 'marry', 'me']\n",
            "['will', 'you', 'marry', 'me'] *\n",
            "['Do', 'you', 'love', 'me']\n",
            "['do', 'you', 'love', 'me'] *\n",
            "['fuck']\n",
            "['fuck'] *\n",
            "['bitch']\n",
            "['bitch'] *\n",
            "['shut', 'up']\n",
            "['shut', 'up'] *\n",
            "['hell']\n",
            "['hell'] *\n",
            "['stupid']\n",
            "['stupid'] *\n",
            "['idiot']\n",
            "['idiot'] *\n",
            "['dumb', 'ass']\n",
            "['dumb', 'as'] *\n",
            "['asshole']\n",
            "['asshole'] *\n",
            "['fucker']\n",
            "['fucker'] *\n",
            "['holidays']\n",
            "['holiday'] *\n",
            "['when', 'will', 'semester', 'starts']\n",
            "['when', 'will', 'semester', 'start'] *\n",
            "['when', 'will', 'semester', 'end']\n",
            "['when', 'will', 'semester', 'end'] *\n",
            "['when', 'is', 'the', 'holidays']\n",
            "['when', 'is', 'the', 'holiday'] *\n",
            "['list', 'of', 'holidays']\n",
            "['list', 'of', 'holiday'] *\n",
            "['Holiday', 'in', 'these', 'year']\n",
            "['holiday', 'in', 'these', 'year'] *\n",
            "['holiday', 'list']\n",
            "['holiday', 'list'] *\n",
            "['about', 'vacations']\n",
            "['about', 'vacation'] *\n",
            "['about', 'holidays']\n",
            "['about', 'holiday'] *\n",
            "['When', 'is', 'vacation']\n",
            "['when', 'is', 'vacation'] *\n",
            "['When', 'is', 'holidays']\n",
            "['when', 'is', 'holiday'] *\n",
            "['how', 'long', 'will', 'be', 'the', 'vacation']\n",
            "['how', 'long', 'will', 'be', 'the', 'vacation'] *\n",
            "['sports', 'and', 'games']\n",
            "['sport', 'and', 'game'] *\n",
            "['give', 'sports', 'details']\n",
            "['give', 'sport', 'detail'] *\n",
            "['sports', 'infrastructure']\n",
            "['sport', 'infrastructure'] *\n",
            "['sports', 'facilities']\n",
            "['sport', 'facility'] *\n",
            "['information', 'about', 'sports']\n",
            "['information', 'about', 'sport'] *\n",
            "['Sports', 'activities']\n",
            "['sport', 'activity'] *\n",
            "['please', 'provide', 'sports', 'and', 'games', 'information']\n",
            "['please', 'provide', 'sport', 'and', 'game', 'information'] *\n",
            "['okk']\n",
            "['okk'] *\n",
            "['okie']\n",
            "['okie'] *\n",
            "['nice', 'work']\n",
            "['nice', 'work'] *\n",
            "['well', 'done']\n",
            "['well', 'done'] *\n",
            "['good', 'job']\n",
            "['good', 'job'] *\n",
            "['thanks', 'for', 'the', 'help']\n",
            "['thanks', 'for', 'the', 'help'] *\n",
            "['Thank', 'You']\n",
            "['thank', 'you'] *\n",
            "['its', 'ok']\n",
            "['it', 'ok'] *\n",
            "['Thanks']\n",
            "['thanks'] *\n",
            "['Good', 'work']\n",
            "['good', 'work'] *\n",
            "['k']\n",
            "['k'] *\n",
            "['ok']\n",
            "['ok'] *\n",
            "['okay']\n",
            "['okay'] *\n",
            "['what', 'can', 'you', 'do']\n",
            "['what', 'can', 'you', 'do'] *\n",
            "['what', 'are', 'the', 'thing', 'you', 'can', 'do']\n",
            "['what', 'are', 'the', 'thing', 'you', 'can', 'do'] *\n",
            "['things', 'you', 'can', 'do']\n",
            "['thing', 'you', 'can', 'do'] *\n",
            "['what', 'can', 'u', 'do', 'for', 'me']\n",
            "['what', 'can', 'u', 'do', 'for', 'me'] *\n",
            "['how', 'u', 'can', 'help', 'me']\n",
            "['how', 'u', 'can', 'help', 'me'] *\n",
            "['why', 'i', 'should', 'use', 'you']\n",
            "['why', 'i', 'should', 'use', 'you'] *\n",
            "['ragging']\n",
            "['ragging'] *\n",
            "['is', 'ragging', 'practice', 'active', 'in', 'college']\n",
            "['is', 'ragging', 'practice', 'active', 'in', 'college'] *\n",
            "['does', 'college', 'have', 'any', 'antiragging', 'facility']\n",
            "['doe', 'college', 'have', 'any', 'antiragging', 'facility'] *\n",
            "['is', 'there', 'any', 'ragging', 'cases']\n",
            "['is', 'there', 'any', 'ragging', 'case'] *\n",
            "['is', 'ragging', 'done', 'here']\n",
            "['is', 'ragging', 'done', 'here'] *\n",
            "['ragging', 'against']\n",
            "['ragging', 'against'] *\n",
            "['antiragging', 'facility']\n",
            "['antiragging', 'facility'] *\n",
            "['ragging', 'juniors']\n",
            "['ragging', 'junior'] *\n",
            "['ragging', 'history']\n",
            "['ragging', 'history'] *\n",
            "['ragging', 'incidents']\n",
            "['ragging', 'incident'] *\n",
            "['hod']\n",
            "['hod'] *\n",
            "['hod', 'name']\n",
            "['hod', 'name'] *\n",
            "['who', 'is', 'the', 'hod']\n",
            "['who', 'is', 'the', 'hod'] *\n"
          ]
        }
      ],
      "source": [
        "data = []  # List to store training data\n",
        "initial = [0] * len(category)  # Initial list of zeros for output row\n",
        "for item in dictonary:\n",
        "    bag = []  # List to store the bag of words representation\n",
        "    word_patterns = item[0]  # Extract the word patterns from the item\n",
        "    print(word_patterns)\n",
        "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]  # Lemmatize the word patterns\n",
        "    print(word_patterns, \"*\")\n",
        "    for vocab_word in vocab:\n",
        "        bag.append(1) if vocab_word in word_patterns else bag.append(0)  # Create the bag of words representation\n",
        "    output_row = list(initial)  # Create a copy of the initial list\n",
        "    output_row[category.index(item[1])] = 1  # Set the appropriate index to 1 for the output row\n",
        "    data.append(bag + output_row)  # Append the bag of words and output row to the training data\n",
        "random.shuffle(data)  # Shuffle the training data\n",
        "data = np.array(data)  # Convert the training data to a numpy array\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)  # Split the data into train and test sets\n",
        "X_train = train_data[:, :len(vocab)]  # Extract the input features for training\n",
        "Y_train = train_data[:, len(vocab):]  # Extract the output labels for training\n",
        "X_test = test_data[:, :len(vocab)]  # Extract the input features for testing\n",
        "Y_test = test_data[:, len(vocab):]  # Extract the output labels for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhWQJNUbx7c5",
        "outputId": "18c28e80-9b74-4ff7-e703-94fe3ac3ef93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-weIe_fSx9i2",
        "outputId": "4cb4c2d7-5e5f-4827-9899-8a95711cb983"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['who', 'is', 'the', 'hod']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgZanZ-uLeHA"
      },
      "source": [
        "## Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8F_VoKIfOke"
      },
      "outputs": [],
      "source": [
        "\"\"\"Lack of Context Understanding: While a bi-LSTM model can capture some context information from both past and future inputs\n",
        "Chatbot conversations often involve complex contextual dependencies, and a bi-LSTM model may struggle to capture all of them effectively.\"\"\"\n",
        "#accuracy 58\n",
        "model_LSTM = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(len(X_train),128),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "  tf.keras.layers.Dense(64, activation='relu'),\n",
        "  tf.keras.layers.Dense(len(Y_train[0]), activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEFWOD7afT7_",
        "outputId": "c161ef6a-e0d0-4596-856b-e5ea8dbe2a67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 6s 147ms/step - loss: 3.6335 - accuracy: 0.0340\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.6072 - accuracy: 0.0710\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 3.5686 - accuracy: 0.0710\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 3.5364 - accuracy: 0.0617\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 3.5195 - accuracy: 0.0617\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 3.5085 - accuracy: 0.0710\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 3.5030 - accuracy: 0.0710\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.4999 - accuracy: 0.0710\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.4961 - accuracy: 0.0586\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 3.4890 - accuracy: 0.0648\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 3.4832 - accuracy: 0.0710\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 2s 176ms/step - loss: 3.4840 - accuracy: 0.0710\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 3.4775 - accuracy: 0.0741\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 2s 175ms/step - loss: 3.4674 - accuracy: 0.0648\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 2s 179ms/step - loss: 3.4582 - accuracy: 0.0772\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 3.4477 - accuracy: 0.0772\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 2s 175ms/step - loss: 3.4356 - accuracy: 0.0772\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 2s 171ms/step - loss: 3.4202 - accuracy: 0.0772\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 3.4047 - accuracy: 0.0772\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 2s 172ms/step - loss: 3.3804 - accuracy: 0.0772\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 3.3427 - accuracy: 0.1019\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 3.3099 - accuracy: 0.1080\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 2s 170ms/step - loss: 3.2834 - accuracy: 0.0988\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 2s 179ms/step - loss: 3.2557 - accuracy: 0.1049\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 3.2395 - accuracy: 0.1142\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 2s 179ms/step - loss: 3.2357 - accuracy: 0.0988\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 2s 176ms/step - loss: 3.1911 - accuracy: 0.1389\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 2s 179ms/step - loss: 3.1527 - accuracy: 0.1481\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 2s 186ms/step - loss: 3.1189 - accuracy: 0.1481\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 3.0964 - accuracy: 0.1574\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 2s 187ms/step - loss: 3.0790 - accuracy: 0.1481\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 2s 212ms/step - loss: 3.0571 - accuracy: 0.1636\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 2s 189ms/step - loss: 3.0362 - accuracy: 0.1451\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 3.0131 - accuracy: 0.1759\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 2s 189ms/step - loss: 3.0003 - accuracy: 0.1698\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 2s 194ms/step - loss: 2.9598 - accuracy: 0.1759\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 2s 216ms/step - loss: 2.9360 - accuracy: 0.1883\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 2.9014 - accuracy: 0.1975\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 2.8734 - accuracy: 0.1914\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 2.8621 - accuracy: 0.2438\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 2.8407 - accuracy: 0.2407\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 2.7989 - accuracy: 0.2377\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 2.7507 - accuracy: 0.2562\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 2.7070 - accuracy: 0.2562\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 2.6949 - accuracy: 0.2593\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 2.6656 - accuracy: 0.2747\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 2.6564 - accuracy: 0.2500\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 2.6096 - accuracy: 0.2654\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 2.5676 - accuracy: 0.2623\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 2.5427 - accuracy: 0.2623\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 2.5006 - accuracy: 0.2809\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 2.5274 - accuracy: 0.2963\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 2.5199 - accuracy: 0.2809\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 2.5088 - accuracy: 0.2901\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 2.5560 - accuracy: 0.2593\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 2.4674 - accuracy: 0.2963\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 3s 230ms/step - loss: 2.4350 - accuracy: 0.2870\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 2s 204ms/step - loss: 2.4189 - accuracy: 0.2994\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 2.3763 - accuracy: 0.3179\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 2.4027 - accuracy: 0.2963\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 2.3974 - accuracy: 0.2778\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 2s 211ms/step - loss: 2.3558 - accuracy: 0.3086\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 2.3199 - accuracy: 0.2840\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 2s 205ms/step - loss: 2.2936 - accuracy: 0.3302\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 2.2955 - accuracy: 0.3395\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 2.2705 - accuracy: 0.3302\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 2.2597 - accuracy: 0.3302\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 2.2393 - accuracy: 0.3426\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 2.4046 - accuracy: 0.2747\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 2.2665 - accuracy: 0.3179\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 2s 205ms/step - loss: 2.2183 - accuracy: 0.3488\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 2.2041 - accuracy: 0.3519\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 2.2047 - accuracy: 0.3549\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 2.1514 - accuracy: 0.3642\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 2.1425 - accuracy: 0.3519\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 2.1255 - accuracy: 0.3519\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 2.1126 - accuracy: 0.3765\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 2.0957 - accuracy: 0.3642\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 2.1389 - accuracy: 0.3395\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 2.1506 - accuracy: 0.3519\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 2.1172 - accuracy: 0.3735\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 2s 193ms/step - loss: 2.1032 - accuracy: 0.3673\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 2.0686 - accuracy: 0.3827\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 2.0514 - accuracy: 0.3858\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 2.0264 - accuracy: 0.3765\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 2s 194ms/step - loss: 2.0182 - accuracy: 0.3920\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 2.0569 - accuracy: 0.3580\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 2.0148 - accuracy: 0.4012\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 2.0226 - accuracy: 0.3765\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 2.0953 - accuracy: 0.3673\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 2s 207ms/step - loss: 2.0724 - accuracy: 0.3704\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 2s 194ms/step - loss: 2.0611 - accuracy: 0.3704\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 2.0378 - accuracy: 0.3920\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 2.0187 - accuracy: 0.3704\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 2s 192ms/step - loss: 1.9725 - accuracy: 0.3981\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 1.9661 - accuracy: 0.3981\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.9489 - accuracy: 0.3981\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.9280 - accuracy: 0.4043\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 1.9002 - accuracy: 0.4228\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.0041 - accuracy: 0.3704\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 1.9005 - accuracy: 0.4259\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 2s 205ms/step - loss: 1.8937 - accuracy: 0.4105\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 1.9248 - accuracy: 0.4105\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 1.8543 - accuracy: 0.4506\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 1.8403 - accuracy: 0.4383\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 2s 209ms/step - loss: 1.8247 - accuracy: 0.4660\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.8207 - accuracy: 0.4444\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 1.9155 - accuracy: 0.4012\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.8440 - accuracy: 0.4012\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 1.8032 - accuracy: 0.4259\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 1.8476 - accuracy: 0.4383\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 1.8424 - accuracy: 0.4290\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 1.8183 - accuracy: 0.4568\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 1.8653 - accuracy: 0.4167\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 1.8427 - accuracy: 0.4383\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 1.8018 - accuracy: 0.4414\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 1.7628 - accuracy: 0.4599\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 2s 206ms/step - loss: 1.7768 - accuracy: 0.4414\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 2s 194ms/step - loss: 1.8137 - accuracy: 0.4136\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.8254 - accuracy: 0.4136\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 1.7955 - accuracy: 0.4136\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.7586 - accuracy: 0.4599\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.6994 - accuracy: 0.4815\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 2s 207ms/step - loss: 1.6893 - accuracy: 0.4969\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 1.6745 - accuracy: 0.4877\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 2s 205ms/step - loss: 1.6834 - accuracy: 0.4815\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 2s 208ms/step - loss: 1.6933 - accuracy: 0.4815\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 2s 205ms/step - loss: 1.6793 - accuracy: 0.4630\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 1.7441 - accuracy: 0.4907\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.7441 - accuracy: 0.4537\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 1.9435 - accuracy: 0.3981\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.8398 - accuracy: 0.4259\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.7673 - accuracy: 0.4321\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 1.7234 - accuracy: 0.4753\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 1.6908 - accuracy: 0.4877\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.6628 - accuracy: 0.5093\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 1.6608 - accuracy: 0.4877\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 2s 193ms/step - loss: 1.6358 - accuracy: 0.4846\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 1.7199 - accuracy: 0.4475\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.7392 - accuracy: 0.4537\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 2s 194ms/step - loss: 1.6638 - accuracy: 0.4630\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 1.6257 - accuracy: 0.4722\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.6037 - accuracy: 0.4907\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 2s 193ms/step - loss: 1.5981 - accuracy: 0.5093\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 2s 209ms/step - loss: 1.5787 - accuracy: 0.5154\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 2s 213ms/step - loss: 1.5701 - accuracy: 0.4969\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 1.5404 - accuracy: 0.5093\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 2s 194ms/step - loss: 1.5540 - accuracy: 0.5185\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 1.5680 - accuracy: 0.5185\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 1.6323 - accuracy: 0.4691\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 1.5925 - accuracy: 0.4969\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 2s 204ms/step - loss: 1.5905 - accuracy: 0.5031\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 1.6873 - accuracy: 0.4784\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 1.6387 - accuracy: 0.4691\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 2s 206ms/step - loss: 1.6005 - accuracy: 0.4877\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 2s 194ms/step - loss: 1.6238 - accuracy: 0.4877\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 1.6267 - accuracy: 0.4846\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.5850 - accuracy: 0.4969\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 1.5083 - accuracy: 0.5340\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 1.4996 - accuracy: 0.5093\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 2s 212ms/step - loss: 1.5601 - accuracy: 0.5000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 1.5378 - accuracy: 0.5031\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.5465 - accuracy: 0.5185\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.5336 - accuracy: 0.5093\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 1.5147 - accuracy: 0.5340\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 1.4715 - accuracy: 0.5340\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.4375 - accuracy: 0.5432\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 1.4264 - accuracy: 0.5586\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 1.4198 - accuracy: 0.5586\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.4027 - accuracy: 0.5617\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 1.3983 - accuracy: 0.5432\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 1.3947 - accuracy: 0.5525\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 1.4192 - accuracy: 0.5525\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 1.4442 - accuracy: 0.5278\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 1.4919 - accuracy: 0.5247\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 1.4489 - accuracy: 0.5340\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.4077 - accuracy: 0.5494\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 1.3725 - accuracy: 0.5463\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 2s 198ms/step - loss: 1.3527 - accuracy: 0.5864\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 1.4124 - accuracy: 0.5309\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 2s 194ms/step - loss: 1.4140 - accuracy: 0.5525\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 1.4489 - accuracy: 0.5309\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 1.3967 - accuracy: 0.5463\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 1.4711 - accuracy: 0.5216\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 2s 200ms/step - loss: 1.4660 - accuracy: 0.5154\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 1.6246 - accuracy: 0.4753\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.4877 - accuracy: 0.5216\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 1.4600 - accuracy: 0.5432\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 1.4357 - accuracy: 0.5340\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 1.4589 - accuracy: 0.5432\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 1.4351 - accuracy: 0.5340\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 2s 197ms/step - loss: 1.3892 - accuracy: 0.5556\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 1.3634 - accuracy: 0.5494\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 1.3601 - accuracy: 0.5648\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 1.3598 - accuracy: 0.5586\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 1.3105 - accuracy: 0.5617\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 2s 210ms/step - loss: 1.3312 - accuracy: 0.5833\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 1.3026 - accuracy: 0.5772\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 2s 207ms/step - loss: 1.2831 - accuracy: 0.5772\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 2s 216ms/step - loss: 1.2981 - accuracy: 0.5772\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x245ff7a5640>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_LSTM.fit(X_train, Y_train, epochs=200, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz_suPTgrT4v",
        "outputId": "078f45ab-a965-4532-ce5c-74e879d51998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 1s 38ms/step - loss: 4.4611 - accuracy: 0.2840\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[4.461057662963867, 0.2839506268501282]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_LSTM.evaluate(X_test,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esQGaIaqNr4r"
      },
      "source": [
        "### GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-w9oxK1rSdk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCm-i6kgLt--"
      },
      "outputs": [],
      "source": [
        "#accuracy 64%\n",
        "model_ = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(len(X_train), 128),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
        "  tf.keras.layers.Dense(64, activation='relu'),\n",
        "  tf.keras.layers.Dense(len(Y_train[0]), activation='sigmoid')\n",
        "])\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2-WX9TmLwyp",
        "outputId": "885e146c-019e-4867-d8e8-9bd5b600571b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "11/11 [==============================] - 6s 106ms/step - loss: 3.6245 - accuracy: 0.0494\n",
            "Epoch 2/250\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 3.5835 - accuracy: 0.0679\n",
            "Epoch 3/250\n",
            "11/11 [==============================] - 2s 170ms/step - loss: 3.5367 - accuracy: 0.0679\n",
            "Epoch 4/250\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 3.5172 - accuracy: 0.0741\n",
            "Epoch 5/250\n",
            "11/11 [==============================] - 2s 186ms/step - loss: 3.4957 - accuracy: 0.0710\n",
            "Epoch 6/250\n",
            "11/11 [==============================] - 2s 192ms/step - loss: 3.4904 - accuracy: 0.0772\n",
            "Epoch 7/250\n",
            "11/11 [==============================] - 2s 177ms/step - loss: 3.4773 - accuracy: 0.0802\n",
            "Epoch 8/250\n",
            "11/11 [==============================] - 2s 173ms/step - loss: 3.4617 - accuracy: 0.0833\n",
            "Epoch 9/250\n",
            "11/11 [==============================] - 2s 180ms/step - loss: 3.4376 - accuracy: 0.0772\n",
            "Epoch 10/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 3.4112 - accuracy: 0.0864\n",
            "Epoch 11/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 3.4008 - accuracy: 0.0864\n",
            "Epoch 12/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 3.3852 - accuracy: 0.0864\n",
            "Epoch 13/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 3.3752 - accuracy: 0.0864\n",
            "Epoch 14/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 3.3651 - accuracy: 0.0864\n",
            "Epoch 15/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 3.3607 - accuracy: 0.0802\n",
            "Epoch 16/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 3.3494 - accuracy: 0.0772\n",
            "Epoch 17/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 3.3465 - accuracy: 0.0864\n",
            "Epoch 18/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 3.3311 - accuracy: 0.0895\n",
            "Epoch 19/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 3.3245 - accuracy: 0.0833\n",
            "Epoch 20/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 3.3126 - accuracy: 0.0988\n",
            "Epoch 21/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 3.2977 - accuracy: 0.1019\n",
            "Epoch 22/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 3.2835 - accuracy: 0.0988\n",
            "Epoch 23/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 3.2693 - accuracy: 0.1019\n",
            "Epoch 24/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 3.2561 - accuracy: 0.1019\n",
            "Epoch 25/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 3.2440 - accuracy: 0.0988\n",
            "Epoch 26/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 3.2243 - accuracy: 0.1080\n",
            "Epoch 27/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 3.1981 - accuracy: 0.1049\n",
            "Epoch 28/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 3.1803 - accuracy: 0.1173\n",
            "Epoch 29/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 3.1521 - accuracy: 0.1173\n",
            "Epoch 30/250\n",
            "11/11 [==============================] - 2s 170ms/step - loss: 3.1467 - accuracy: 0.1389\n",
            "Epoch 31/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 3.1130 - accuracy: 0.1358\n",
            "Epoch 32/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 3.0898 - accuracy: 0.1327\n",
            "Epoch 33/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 3.0836 - accuracy: 0.1420\n",
            "Epoch 34/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 3.0659 - accuracy: 0.1512\n",
            "Epoch 35/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 3.0430 - accuracy: 0.1543\n",
            "Epoch 36/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 3.0366 - accuracy: 0.1327\n",
            "Epoch 37/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 3.0148 - accuracy: 0.1481\n",
            "Epoch 38/250\n",
            "11/11 [==============================] - 2s 192ms/step - loss: 2.9916 - accuracy: 0.1883\n",
            "Epoch 39/250\n",
            "11/11 [==============================] - 2s 170ms/step - loss: 2.9768 - accuracy: 0.2037\n",
            "Epoch 40/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.9330 - accuracy: 0.2160\n",
            "Epoch 41/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.8948 - accuracy: 0.2068\n",
            "Epoch 42/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 2.8620 - accuracy: 0.2037\n",
            "Epoch 43/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 2.8190 - accuracy: 0.2068\n",
            "Epoch 44/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 2.7839 - accuracy: 0.2191\n",
            "Epoch 45/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.8030 - accuracy: 0.1944\n",
            "Epoch 46/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 2.7388 - accuracy: 0.2284\n",
            "Epoch 47/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.7182 - accuracy: 0.2346\n",
            "Epoch 48/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 2.7157 - accuracy: 0.2315\n",
            "Epoch 49/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 2.6975 - accuracy: 0.2469\n",
            "Epoch 50/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 2.6677 - accuracy: 0.2623\n",
            "Epoch 51/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 2.6555 - accuracy: 0.2377\n",
            "Epoch 52/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 2.6390 - accuracy: 0.2346\n",
            "Epoch 53/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 2.6466 - accuracy: 0.2407\n",
            "Epoch 54/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.6367 - accuracy: 0.2438\n",
            "Epoch 55/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 2.6195 - accuracy: 0.2407\n",
            "Epoch 56/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.6219 - accuracy: 0.2222\n",
            "Epoch 57/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.5927 - accuracy: 0.2438\n",
            "Epoch 58/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.5861 - accuracy: 0.2562\n",
            "Epoch 59/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.5744 - accuracy: 0.2562\n",
            "Epoch 60/250\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 2.5680 - accuracy: 0.2531\n",
            "Epoch 61/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 2.5595 - accuracy: 0.2685\n",
            "Epoch 62/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 2.5395 - accuracy: 0.2654\n",
            "Epoch 63/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.5391 - accuracy: 0.2685\n",
            "Epoch 64/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 2.5243 - accuracy: 0.2809\n",
            "Epoch 65/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 2.5141 - accuracy: 0.2809\n",
            "Epoch 66/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.5010 - accuracy: 0.2932\n",
            "Epoch 67/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 2.5067 - accuracy: 0.2963\n",
            "Epoch 68/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 2.5007 - accuracy: 0.2963\n",
            "Epoch 69/250\n",
            "11/11 [==============================] - 2s 170ms/step - loss: 2.4664 - accuracy: 0.2994\n",
            "Epoch 70/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 2.4515 - accuracy: 0.3056\n",
            "Epoch 71/250\n",
            "11/11 [==============================] - 2s 173ms/step - loss: 2.6624 - accuracy: 0.2222\n",
            "Epoch 72/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 2.4746 - accuracy: 0.2623\n",
            "Epoch 73/250\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 2.4586 - accuracy: 0.2932\n",
            "Epoch 74/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 2.4054 - accuracy: 0.3179\n",
            "Epoch 75/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 2.4163 - accuracy: 0.3025\n",
            "Epoch 76/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 2.4081 - accuracy: 0.3056\n",
            "Epoch 77/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 2.3937 - accuracy: 0.2840\n",
            "Epoch 78/250\n",
            "11/11 [==============================] - 2s 172ms/step - loss: 2.3851 - accuracy: 0.3086\n",
            "Epoch 79/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 2.3579 - accuracy: 0.3210\n",
            "Epoch 80/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 2.3586 - accuracy: 0.3210\n",
            "Epoch 81/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.3595 - accuracy: 0.3272\n",
            "Epoch 82/250\n",
            "11/11 [==============================] - 2s 161ms/step - loss: 2.3390 - accuracy: 0.3241\n",
            "Epoch 83/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.3221 - accuracy: 0.3210\n",
            "Epoch 84/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.3220 - accuracy: 0.3364\n",
            "Epoch 85/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 2.3040 - accuracy: 0.3272\n",
            "Epoch 86/250\n",
            "11/11 [==============================] - 2s 161ms/step - loss: 2.3013 - accuracy: 0.3241\n",
            "Epoch 87/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.3038 - accuracy: 0.3241\n",
            "Epoch 88/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.2974 - accuracy: 0.3179\n",
            "Epoch 89/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.2883 - accuracy: 0.3272\n",
            "Epoch 90/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 2.2526 - accuracy: 0.3395\n",
            "Epoch 91/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.2444 - accuracy: 0.3272\n",
            "Epoch 92/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.2364 - accuracy: 0.3457\n",
            "Epoch 93/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.2293 - accuracy: 0.3395\n",
            "Epoch 94/250\n",
            "11/11 [==============================] - 2s 178ms/step - loss: 2.2076 - accuracy: 0.3611\n",
            "Epoch 95/250\n",
            "11/11 [==============================] - 2s 182ms/step - loss: 2.2022 - accuracy: 0.3457\n",
            "Epoch 96/250\n",
            "11/11 [==============================] - 2s 175ms/step - loss: 2.2114 - accuracy: 0.3457\n",
            "Epoch 97/250\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 2.2130 - accuracy: 0.3364\n",
            "Epoch 98/250\n",
            "11/11 [==============================] - 2s 179ms/step - loss: 2.1795 - accuracy: 0.3333\n",
            "Epoch 99/250\n",
            "11/11 [==============================] - 2s 173ms/step - loss: 2.1764 - accuracy: 0.3611\n",
            "Epoch 100/250\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 2.2964 - accuracy: 0.2932\n",
            "Epoch 101/250\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 2.4947 - accuracy: 0.2377\n",
            "Epoch 102/250\n",
            "11/11 [==============================] - 2s 170ms/step - loss: 2.2994 - accuracy: 0.2932\n",
            "Epoch 103/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.2071 - accuracy: 0.3333\n",
            "Epoch 104/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 2.2057 - accuracy: 0.3426\n",
            "Epoch 105/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 2.2224 - accuracy: 0.3272\n",
            "Epoch 106/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 2.2997 - accuracy: 0.2778\n",
            "Epoch 107/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 2.2052 - accuracy: 0.3364\n",
            "Epoch 108/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 2.1385 - accuracy: 0.3549\n",
            "Epoch 109/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 2.1417 - accuracy: 0.3549\n",
            "Epoch 110/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.1547 - accuracy: 0.3333\n",
            "Epoch 111/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 2.1099 - accuracy: 0.3580\n",
            "Epoch 112/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 2.0998 - accuracy: 0.3519\n",
            "Epoch 113/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 2.0899 - accuracy: 0.3580\n",
            "Epoch 114/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 2.0802 - accuracy: 0.3580\n",
            "Epoch 115/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 2.0659 - accuracy: 0.3765\n",
            "Epoch 116/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 2.0716 - accuracy: 0.3735\n",
            "Epoch 117/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.0562 - accuracy: 0.3549\n",
            "Epoch 118/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 2.0560 - accuracy: 0.3796\n",
            "Epoch 119/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 2.0301 - accuracy: 0.3827\n",
            "Epoch 120/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 2.0306 - accuracy: 0.3827\n",
            "Epoch 121/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.0282 - accuracy: 0.3796\n",
            "Epoch 122/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 2.0283 - accuracy: 0.3642\n",
            "Epoch 123/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 2.1395 - accuracy: 0.3241\n",
            "Epoch 124/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 2.0659 - accuracy: 0.3580\n",
            "Epoch 125/250\n",
            "11/11 [==============================] - 2s 180ms/step - loss: 2.0104 - accuracy: 0.3951\n",
            "Epoch 126/250\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 2.0143 - accuracy: 0.3642\n",
            "Epoch 127/250\n",
            "11/11 [==============================] - 2s 173ms/step - loss: 2.0225 - accuracy: 0.3673\n",
            "Epoch 128/250\n",
            "11/11 [==============================] - 2s 173ms/step - loss: 2.0229 - accuracy: 0.3611\n",
            "Epoch 129/250\n",
            "11/11 [==============================] - 2s 172ms/step - loss: 1.9922 - accuracy: 0.3611\n",
            "Epoch 130/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.9642 - accuracy: 0.3765\n",
            "Epoch 131/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 1.9588 - accuracy: 0.3889\n",
            "Epoch 132/250\n",
            "11/11 [==============================] - 2s 170ms/step - loss: 1.9788 - accuracy: 0.3704\n",
            "Epoch 133/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.9709 - accuracy: 0.4012\n",
            "Epoch 134/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 1.9515 - accuracy: 0.3827\n",
            "Epoch 135/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.9519 - accuracy: 0.3765\n",
            "Epoch 136/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 1.9495 - accuracy: 0.3765\n",
            "Epoch 137/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.9157 - accuracy: 0.3981\n",
            "Epoch 138/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.9021 - accuracy: 0.4012\n",
            "Epoch 139/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.9110 - accuracy: 0.3981\n",
            "Epoch 140/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.9093 - accuracy: 0.3981\n",
            "Epoch 141/250\n",
            "11/11 [==============================] - 2s 171ms/step - loss: 1.9094 - accuracy: 0.3920\n",
            "Epoch 142/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 1.8865 - accuracy: 0.4259\n",
            "Epoch 143/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 1.9392 - accuracy: 0.3735\n",
            "Epoch 144/250\n",
            "11/11 [==============================] - 2s 171ms/step - loss: 2.0195 - accuracy: 0.3549\n",
            "Epoch 145/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.9006 - accuracy: 0.3858\n",
            "Epoch 146/250\n",
            "11/11 [==============================] - 2s 171ms/step - loss: 1.8607 - accuracy: 0.4167\n",
            "Epoch 147/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.8307 - accuracy: 0.4198\n",
            "Epoch 148/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.8407 - accuracy: 0.3889\n",
            "Epoch 149/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.8312 - accuracy: 0.3981\n",
            "Epoch 150/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.8533 - accuracy: 0.4198\n",
            "Epoch 151/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 1.8934 - accuracy: 0.3611\n",
            "Epoch 152/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.0310 - accuracy: 0.3488\n",
            "Epoch 153/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.9094 - accuracy: 0.3827\n",
            "Epoch 154/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.9076 - accuracy: 0.3981\n",
            "Epoch 155/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.9268 - accuracy: 0.3796\n",
            "Epoch 156/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.9067 - accuracy: 0.4105\n",
            "Epoch 157/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.8338 - accuracy: 0.4228\n",
            "Epoch 158/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.8170 - accuracy: 0.4105\n",
            "Epoch 159/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.7883 - accuracy: 0.4136\n",
            "Epoch 160/250\n",
            "11/11 [==============================] - 2s 171ms/step - loss: 1.7810 - accuracy: 0.4259\n",
            "Epoch 161/250\n",
            "11/11 [==============================] - 2s 177ms/step - loss: 1.8363 - accuracy: 0.4383\n",
            "Epoch 162/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.8477 - accuracy: 0.3981\n",
            "Epoch 163/250\n",
            "11/11 [==============================] - 2s 161ms/step - loss: 1.9548 - accuracy: 0.3889\n",
            "Epoch 164/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.8656 - accuracy: 0.3796\n",
            "Epoch 165/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.7708 - accuracy: 0.4414\n",
            "Epoch 166/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 1.7442 - accuracy: 0.4537\n",
            "Epoch 167/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.7617 - accuracy: 0.4290\n",
            "Epoch 168/250\n",
            "11/11 [==============================] - 2s 161ms/step - loss: 1.7912 - accuracy: 0.4259\n",
            "Epoch 169/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.7412 - accuracy: 0.4352\n",
            "Epoch 170/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 1.7062 - accuracy: 0.4568\n",
            "Epoch 171/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.7079 - accuracy: 0.4383\n",
            "Epoch 172/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.7327 - accuracy: 0.4105\n",
            "Epoch 173/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.7176 - accuracy: 0.4599\n",
            "Epoch 174/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.6913 - accuracy: 0.4599\n",
            "Epoch 175/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 1.6644 - accuracy: 0.4383\n",
            "Epoch 176/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.6671 - accuracy: 0.4630\n",
            "Epoch 177/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.6897 - accuracy: 0.4506\n",
            "Epoch 178/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.6640 - accuracy: 0.4568\n",
            "Epoch 179/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 1.6357 - accuracy: 0.4691\n",
            "Epoch 180/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.6600 - accuracy: 0.4537\n",
            "Epoch 181/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 1.6747 - accuracy: 0.4599\n",
            "Epoch 182/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.6940 - accuracy: 0.4599\n",
            "Epoch 183/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.6694 - accuracy: 0.4660\n",
            "Epoch 184/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 1.6404 - accuracy: 0.4784\n",
            "Epoch 185/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.6287 - accuracy: 0.4599\n",
            "Epoch 186/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.6611 - accuracy: 0.4537\n",
            "Epoch 187/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.5977 - accuracy: 0.4568\n",
            "Epoch 188/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 1.6272 - accuracy: 0.4691\n",
            "Epoch 189/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 1.6153 - accuracy: 0.4722\n",
            "Epoch 190/250\n",
            "11/11 [==============================] - 2s 172ms/step - loss: 1.5894 - accuracy: 0.4599\n",
            "Epoch 191/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.5600 - accuracy: 0.4938\n",
            "Epoch 192/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.5549 - accuracy: 0.4938\n",
            "Epoch 193/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.6228 - accuracy: 0.4753\n",
            "Epoch 194/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.9490 - accuracy: 0.3642\n",
            "Epoch 195/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.6541 - accuracy: 0.4630\n",
            "Epoch 196/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 1.6523 - accuracy: 0.4630\n",
            "Epoch 197/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.5721 - accuracy: 0.4938\n",
            "Epoch 198/250\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 1.6109 - accuracy: 0.4630\n",
            "Epoch 199/250\n",
            "11/11 [==============================] - 2s 171ms/step - loss: 1.5860 - accuracy: 0.4722\n",
            "Epoch 200/250\n",
            "11/11 [==============================] - 2s 161ms/step - loss: 1.6038 - accuracy: 0.4784\n",
            "Epoch 201/250\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 1.6215 - accuracy: 0.4691\n",
            "Epoch 202/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 1.5362 - accuracy: 0.4722\n",
            "Epoch 203/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.5654 - accuracy: 0.4938\n",
            "Epoch 204/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.5386 - accuracy: 0.5000\n",
            "Epoch 205/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.5987 - accuracy: 0.4537\n",
            "Epoch 206/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.5250 - accuracy: 0.4907\n",
            "Epoch 207/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.4985 - accuracy: 0.5000\n",
            "Epoch 208/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 1.5281 - accuracy: 0.4877\n",
            "Epoch 209/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.5027 - accuracy: 0.4938\n",
            "Epoch 210/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.4937 - accuracy: 0.4969\n",
            "Epoch 211/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.4801 - accuracy: 0.5123\n",
            "Epoch 212/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.4640 - accuracy: 0.4969\n",
            "Epoch 213/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.4879 - accuracy: 0.5123\n",
            "Epoch 214/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.5910 - accuracy: 0.4877\n",
            "Epoch 215/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 2.1869 - accuracy: 0.3488\n",
            "Epoch 216/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 1.9729 - accuracy: 0.3673\n",
            "Epoch 217/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 1.7040 - accuracy: 0.4167\n",
            "Epoch 218/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.5677 - accuracy: 0.4846\n",
            "Epoch 219/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 1.5205 - accuracy: 0.5093\n",
            "Epoch 220/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.5271 - accuracy: 0.5000\n",
            "Epoch 221/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.5096 - accuracy: 0.5000\n",
            "Epoch 222/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 1.4682 - accuracy: 0.4877\n",
            "Epoch 223/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.4506 - accuracy: 0.4938\n",
            "Epoch 224/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 1.4426 - accuracy: 0.5185\n",
            "Epoch 225/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.4772 - accuracy: 0.5000\n",
            "Epoch 226/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.4741 - accuracy: 0.4969\n",
            "Epoch 227/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.4762 - accuracy: 0.4877\n",
            "Epoch 228/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.4900 - accuracy: 0.5000\n",
            "Epoch 229/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.4254 - accuracy: 0.5154\n",
            "Epoch 230/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.4226 - accuracy: 0.5185\n",
            "Epoch 231/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 1.4092 - accuracy: 0.5093\n",
            "Epoch 232/250\n",
            "11/11 [==============================] - 2s 180ms/step - loss: 1.3956 - accuracy: 0.5401\n",
            "Epoch 233/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 1.3983 - accuracy: 0.5216\n",
            "Epoch 234/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 1.3821 - accuracy: 0.5494\n",
            "Epoch 235/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 1.5001 - accuracy: 0.5062\n",
            "Epoch 236/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.4041 - accuracy: 0.5247\n",
            "Epoch 237/250\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 1.3567 - accuracy: 0.5309\n",
            "Epoch 238/250\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 1.3896 - accuracy: 0.5247\n",
            "Epoch 239/250\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.4151 - accuracy: 0.4877\n",
            "Epoch 240/250\n",
            "11/11 [==============================] - 2s 159ms/step - loss: 1.3706 - accuracy: 0.5216\n",
            "Epoch 241/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.3744 - accuracy: 0.5401\n",
            "Epoch 242/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.3561 - accuracy: 0.5525\n",
            "Epoch 243/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.3324 - accuracy: 0.5556\n",
            "Epoch 244/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.3239 - accuracy: 0.5370\n",
            "Epoch 245/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.3121 - accuracy: 0.5463\n",
            "Epoch 246/250\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 1.3182 - accuracy: 0.5617\n",
            "Epoch 247/250\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 1.3165 - accuracy: 0.5648\n",
            "Epoch 248/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.3174 - accuracy: 0.5525\n",
            "Epoch 249/250\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.3360 - accuracy: 0.5463\n",
            "Epoch 250/250\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 1.3030 - accuracy: 0.5772\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x245863bb730>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_.fit(X_train, Y_train, epochs=250, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f29bWdlxtt0g",
        "outputId": "e48af4ff-f26f-4bd7-b9ba-26911bf40d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 1s 27ms/step - loss: 5.1524 - accuracy: 0.2963\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[5.152417182922363, 0.29629629850387573]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_.evaluate(X_test,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T7fTj02TVvj"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrUj09lomwgI"
      },
      "outputs": [],
      "source": [
        "#accuracy 50%\n",
        "m = tf.keras.Sequential()\n",
        "m.add(tf.keras.layers.Embedding(len(X_train), 64)),\n",
        "m.add(tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\", kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_regularizer=tf.keras.regularizers.L2(0.0001), kernel_regularizer=tf.keras.regularizers.L2(0.0001), activity_regularizer = tf.keras.regularizers.L2(0.0001)))\n",
        "m.add(tf.keras.layers.Dropout(0.3))\n",
        "m.add(tf.keras.layers.LSTM(32, dropout=0.3,return_sequences=True))\n",
        "m.add(tf.keras.layers.LSTM(16, dropout=0.3,return_sequences=False))\n",
        "m.add(tf.keras.layers.Dense(128,activation=\"relu\", activity_regularizer = tf.keras.regularizers.L2(0.0001)))\n",
        "m.add(tf.keras.layers.Dropout(0.6))\n",
        "m.add(tf.keras.layers.Dense(len(Y_train[0]), activation=\"softmax\", activity_regularizer = tf.keras.regularizers.L2(0.0001)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTgrBp0Gmwjb",
        "outputId": "291a9aab-8e9a-4b96-a0d1-6dd1bf688072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "11/11 [==============================] - 7s 169ms/step - loss: 3.6393 - accuracy: 0.0463\n",
            "Epoch 2/250\n",
            "11/11 [==============================] - 2s 209ms/step - loss: 3.6226 - accuracy: 0.0617\n",
            "Epoch 3/250\n",
            "11/11 [==============================] - 2s 161ms/step - loss: 3.5971 - accuracy: 0.0494\n",
            "Epoch 4/250\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 3.5731 - accuracy: 0.0802\n",
            "Epoch 5/250\n",
            "11/11 [==============================] - 3s 241ms/step - loss: 3.5512 - accuracy: 0.0525\n",
            "Epoch 6/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 3.5161 - accuracy: 0.0833\n",
            "Epoch 7/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 3.5235 - accuracy: 0.0741\n",
            "Epoch 8/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 3.5183 - accuracy: 0.0586\n",
            "Epoch 9/250\n",
            "11/11 [==============================] - 3s 227ms/step - loss: 3.5181 - accuracy: 0.0586\n",
            "Epoch 10/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 3.5101 - accuracy: 0.0463\n",
            "Epoch 11/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 3.5093 - accuracy: 0.0741\n",
            "Epoch 12/250\n",
            "11/11 [==============================] - 2s 227ms/step - loss: 3.4987 - accuracy: 0.0648\n",
            "Epoch 13/250\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 3.5246 - accuracy: 0.0586\n",
            "Epoch 14/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 3.5192 - accuracy: 0.0833\n",
            "Epoch 15/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 3.5106 - accuracy: 0.0648\n",
            "Epoch 16/250\n",
            "11/11 [==============================] - 3s 236ms/step - loss: 3.5210 - accuracy: 0.0494\n",
            "Epoch 17/250\n",
            "11/11 [==============================] - 2s 217ms/step - loss: 3.5130 - accuracy: 0.0556\n",
            "Epoch 18/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 3.4881 - accuracy: 0.0864\n",
            "Epoch 19/250\n",
            "11/11 [==============================] - 3s 227ms/step - loss: 3.5057 - accuracy: 0.0772\n",
            "Epoch 20/250\n",
            "11/11 [==============================] - 2s 227ms/step - loss: 3.5262 - accuracy: 0.0741\n",
            "Epoch 21/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 3.5093 - accuracy: 0.0772\n",
            "Epoch 22/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 3.4883 - accuracy: 0.0710\n",
            "Epoch 23/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 3.4940 - accuracy: 0.0648\n",
            "Epoch 24/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 3.5109 - accuracy: 0.0895\n",
            "Epoch 25/250\n",
            "11/11 [==============================] - 2s 228ms/step - loss: 3.4906 - accuracy: 0.0741\n",
            "Epoch 26/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 3.5066 - accuracy: 0.0556\n",
            "Epoch 27/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 3.5267 - accuracy: 0.0432\n",
            "Epoch 28/250\n",
            "11/11 [==============================] - 3s 224ms/step - loss: 3.5090 - accuracy: 0.0556\n",
            "Epoch 29/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 3.4914 - accuracy: 0.0772\n",
            "Epoch 30/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 3.4924 - accuracy: 0.0772\n",
            "Epoch 31/250\n",
            "11/11 [==============================] - 2s 227ms/step - loss: 3.4747 - accuracy: 0.0525\n",
            "Epoch 32/250\n",
            "11/11 [==============================] - 3s 244ms/step - loss: 3.4934 - accuracy: 0.0864\n",
            "Epoch 33/250\n",
            "11/11 [==============================] - 3s 227ms/step - loss: 3.4758 - accuracy: 0.0525\n",
            "Epoch 34/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 3.4775 - accuracy: 0.0741\n",
            "Epoch 35/250\n",
            "11/11 [==============================] - 3s 229ms/step - loss: 3.4833 - accuracy: 0.0833\n",
            "Epoch 36/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 3.4974 - accuracy: 0.0741\n",
            "Epoch 37/250\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 3.4889 - accuracy: 0.0741\n",
            "Epoch 38/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 3.4988 - accuracy: 0.0679\n",
            "Epoch 39/250\n",
            "11/11 [==============================] - 3s 233ms/step - loss: 3.4864 - accuracy: 0.0617\n",
            "Epoch 40/250\n",
            "11/11 [==============================] - 3s 237ms/step - loss: 3.4679 - accuracy: 0.0679\n",
            "Epoch 41/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 3.4227 - accuracy: 0.0772\n",
            "Epoch 42/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 3.4237 - accuracy: 0.0648\n",
            "Epoch 43/250\n",
            "11/11 [==============================] - 3s 226ms/step - loss: 3.4066 - accuracy: 0.0895\n",
            "Epoch 44/250\n",
            "11/11 [==============================] - 2s 228ms/step - loss: 3.3722 - accuracy: 0.1142\n",
            "Epoch 45/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 3.3723 - accuracy: 0.0741\n",
            "Epoch 46/250\n",
            "11/11 [==============================] - 3s 228ms/step - loss: 3.3588 - accuracy: 0.1142\n",
            "Epoch 47/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 3.3503 - accuracy: 0.1049\n",
            "Epoch 48/250\n",
            "11/11 [==============================] - 2s 227ms/step - loss: 3.3157 - accuracy: 0.0864\n",
            "Epoch 49/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 3.3258 - accuracy: 0.1019\n",
            "Epoch 50/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 3.3041 - accuracy: 0.0864\n",
            "Epoch 51/250\n",
            "11/11 [==============================] - 3s 225ms/step - loss: 3.2830 - accuracy: 0.1080\n",
            "Epoch 52/250\n",
            "11/11 [==============================] - 3s 238ms/step - loss: 3.3072 - accuracy: 0.0802\n",
            "Epoch 53/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 3.2655 - accuracy: 0.0988\n",
            "Epoch 54/250\n",
            "11/11 [==============================] - 3s 246ms/step - loss: 3.2760 - accuracy: 0.1142\n",
            "Epoch 55/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 3.2735 - accuracy: 0.1080\n",
            "Epoch 56/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 3.2880 - accuracy: 0.0833\n",
            "Epoch 57/250\n",
            "11/11 [==============================] - 3s 231ms/step - loss: 3.2735 - accuracy: 0.0741\n",
            "Epoch 58/250\n",
            "11/11 [==============================] - 3s 229ms/step - loss: 3.2491 - accuracy: 0.1142\n",
            "Epoch 59/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 3.2388 - accuracy: 0.0833\n",
            "Epoch 60/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 3.2349 - accuracy: 0.1204\n",
            "Epoch 61/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 3.2229 - accuracy: 0.1173\n",
            "Epoch 62/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 3.2216 - accuracy: 0.0957\n",
            "Epoch 63/250\n",
            "11/11 [==============================] - 3s 227ms/step - loss: 3.1948 - accuracy: 0.1111\n",
            "Epoch 64/250\n",
            "11/11 [==============================] - 3s 234ms/step - loss: 3.1850 - accuracy: 0.0988\n",
            "Epoch 65/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 3.1777 - accuracy: 0.1080\n",
            "Epoch 66/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 3.1828 - accuracy: 0.1142\n",
            "Epoch 67/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 3.1760 - accuracy: 0.1204\n",
            "Epoch 68/250\n",
            "11/11 [==============================] - 3s 231ms/step - loss: 3.1837 - accuracy: 0.1265\n",
            "Epoch 69/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 3.1664 - accuracy: 0.0895\n",
            "Epoch 70/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 3.1309 - accuracy: 0.1358\n",
            "Epoch 71/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 3.1568 - accuracy: 0.1173\n",
            "Epoch 72/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 3.1588 - accuracy: 0.1019\n",
            "Epoch 73/250\n",
            "11/11 [==============================] - 3s 230ms/step - loss: 3.1199 - accuracy: 0.1111\n",
            "Epoch 74/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 3.1304 - accuracy: 0.1605\n",
            "Epoch 75/250\n",
            "11/11 [==============================] - 3s 232ms/step - loss: 3.1242 - accuracy: 0.1049\n",
            "Epoch 76/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 3.1035 - accuracy: 0.1327\n",
            "Epoch 77/250\n",
            "11/11 [==============================] - 2s 227ms/step - loss: 3.0823 - accuracy: 0.1389\n",
            "Epoch 78/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 3.1238 - accuracy: 0.1173\n",
            "Epoch 79/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 3.0820 - accuracy: 0.1451\n",
            "Epoch 80/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 3.0699 - accuracy: 0.1574\n",
            "Epoch 81/250\n",
            "11/11 [==============================] - 2s 228ms/step - loss: 3.0361 - accuracy: 0.1543\n",
            "Epoch 82/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 3.0784 - accuracy: 0.1451\n",
            "Epoch 83/250\n",
            "11/11 [==============================] - 3s 249ms/step - loss: 3.0460 - accuracy: 0.1358\n",
            "Epoch 84/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 3.0454 - accuracy: 0.1481\n",
            "Epoch 85/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 3.0490 - accuracy: 0.1574\n",
            "Epoch 86/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 2.9904 - accuracy: 0.1605\n",
            "Epoch 87/250\n",
            "11/11 [==============================] - 2s 228ms/step - loss: 3.0088 - accuracy: 0.1327\n",
            "Epoch 88/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 2.9980 - accuracy: 0.1574\n",
            "Epoch 89/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 3.0360 - accuracy: 0.1605\n",
            "Epoch 90/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.9955 - accuracy: 0.1512\n",
            "Epoch 91/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.9885 - accuracy: 0.1420\n",
            "Epoch 92/250\n",
            "11/11 [==============================] - 2s 215ms/step - loss: 2.9854 - accuracy: 0.1543\n",
            "Epoch 93/250\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 3.0044 - accuracy: 0.1327\n",
            "Epoch 94/250\n",
            "11/11 [==============================] - 2s 215ms/step - loss: 2.9701 - accuracy: 0.1451\n",
            "Epoch 95/250\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 2.9819 - accuracy: 0.1698\n",
            "Epoch 96/250\n",
            "11/11 [==============================] - 2s 216ms/step - loss: 2.9693 - accuracy: 0.1605\n",
            "Epoch 97/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.9819 - accuracy: 0.1728\n",
            "Epoch 98/250\n",
            "11/11 [==============================] - 2s 216ms/step - loss: 2.9495 - accuracy: 0.1667\n",
            "Epoch 99/250\n",
            "11/11 [==============================] - 3s 238ms/step - loss: 2.9383 - accuracy: 0.1698\n",
            "Epoch 100/250\n",
            "11/11 [==============================] - 3s 231ms/step - loss: 2.9416 - accuracy: 0.1944\n",
            "Epoch 101/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.8805 - accuracy: 0.2006\n",
            "Epoch 102/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.8813 - accuracy: 0.1944\n",
            "Epoch 103/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.8609 - accuracy: 0.1975\n",
            "Epoch 104/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.8950 - accuracy: 0.2191\n",
            "Epoch 105/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.9641 - accuracy: 0.2006\n",
            "Epoch 106/250\n",
            "11/11 [==============================] - 3s 232ms/step - loss: 2.9026 - accuracy: 0.1914\n",
            "Epoch 107/250\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 2.8810 - accuracy: 0.2068\n",
            "Epoch 108/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.8697 - accuracy: 0.1975\n",
            "Epoch 109/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.8442 - accuracy: 0.2160\n",
            "Epoch 110/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.8481 - accuracy: 0.1883\n",
            "Epoch 111/250\n",
            "11/11 [==============================] - 3s 224ms/step - loss: 2.8372 - accuracy: 0.1914\n",
            "Epoch 112/250\n",
            "11/11 [==============================] - 3s 222ms/step - loss: 2.8044 - accuracy: 0.2099\n",
            "Epoch 113/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.8553 - accuracy: 0.1975\n",
            "Epoch 114/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.8263 - accuracy: 0.2099\n",
            "Epoch 115/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.8221 - accuracy: 0.2037\n",
            "Epoch 116/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.7837 - accuracy: 0.2222\n",
            "Epoch 117/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.7604 - accuracy: 0.2099\n",
            "Epoch 118/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.7730 - accuracy: 0.2315\n",
            "Epoch 119/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.7554 - accuracy: 0.2006\n",
            "Epoch 120/250\n",
            "11/11 [==============================] - 2s 228ms/step - loss: 2.7818 - accuracy: 0.2099\n",
            "Epoch 121/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 2.7147 - accuracy: 0.2284\n",
            "Epoch 122/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.7501 - accuracy: 0.2130\n",
            "Epoch 123/250\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 2.6882 - accuracy: 0.2438\n",
            "Epoch 124/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.7533 - accuracy: 0.2222\n",
            "Epoch 125/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 2.6935 - accuracy: 0.2315\n",
            "Epoch 126/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.7260 - accuracy: 0.1975\n",
            "Epoch 127/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.7042 - accuracy: 0.2377\n",
            "Epoch 128/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.7172 - accuracy: 0.2315\n",
            "Epoch 129/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.6831 - accuracy: 0.2469\n",
            "Epoch 130/250\n",
            "11/11 [==============================] - 3s 227ms/step - loss: 2.6788 - accuracy: 0.2407\n",
            "Epoch 131/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.7152 - accuracy: 0.2191\n",
            "Epoch 132/250\n",
            "11/11 [==============================] - 3s 230ms/step - loss: 2.7162 - accuracy: 0.2191\n",
            "Epoch 133/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.6468 - accuracy: 0.2531\n",
            "Epoch 134/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 2.6878 - accuracy: 0.2315\n",
            "Epoch 135/250\n",
            "11/11 [==============================] - 3s 230ms/step - loss: 2.6726 - accuracy: 0.2160\n",
            "Epoch 136/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.6769 - accuracy: 0.2531\n",
            "Epoch 137/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 2.6429 - accuracy: 0.2377\n",
            "Epoch 138/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.6740 - accuracy: 0.2377\n",
            "Epoch 139/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.6259 - accuracy: 0.2531\n",
            "Epoch 140/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.6234 - accuracy: 0.2500\n",
            "Epoch 141/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.6681 - accuracy: 0.2531\n",
            "Epoch 142/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 2.5874 - accuracy: 0.2623\n",
            "Epoch 143/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.6450 - accuracy: 0.2531\n",
            "Epoch 144/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.5639 - accuracy: 0.2654\n",
            "Epoch 145/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.6076 - accuracy: 0.2500\n",
            "Epoch 146/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.6371 - accuracy: 0.2593\n",
            "Epoch 147/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.5873 - accuracy: 0.2500\n",
            "Epoch 148/250\n",
            "11/11 [==============================] - 3s 233ms/step - loss: 2.6115 - accuracy: 0.2315\n",
            "Epoch 149/250\n",
            "11/11 [==============================] - 3s 235ms/step - loss: 2.6075 - accuracy: 0.2438\n",
            "Epoch 150/250\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 2.7014 - accuracy: 0.2407\n",
            "Epoch 151/250\n",
            "11/11 [==============================] - 3s 229ms/step - loss: 2.7137 - accuracy: 0.2099\n",
            "Epoch 152/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.6543 - accuracy: 0.2531\n",
            "Epoch 153/250\n",
            "11/11 [==============================] - 3s 230ms/step - loss: 2.6298 - accuracy: 0.2469\n",
            "Epoch 154/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.5749 - accuracy: 0.2685\n",
            "Epoch 155/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.5795 - accuracy: 0.2593\n",
            "Epoch 156/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.5368 - accuracy: 0.2623\n",
            "Epoch 157/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.5610 - accuracy: 0.2747\n",
            "Epoch 158/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.5760 - accuracy: 0.2593\n",
            "Epoch 159/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.5496 - accuracy: 0.2840\n",
            "Epoch 160/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.5783 - accuracy: 0.2685\n",
            "Epoch 161/250\n",
            "11/11 [==============================] - 2s 228ms/step - loss: 2.5506 - accuracy: 0.2593\n",
            "Epoch 162/250\n",
            "11/11 [==============================] - 3s 241ms/step - loss: 2.5584 - accuracy: 0.2809\n",
            "Epoch 163/250\n",
            "11/11 [==============================] - 3s 230ms/step - loss: 2.5377 - accuracy: 0.2469\n",
            "Epoch 164/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.5402 - accuracy: 0.2747\n",
            "Epoch 165/250\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 2.5785 - accuracy: 0.2685\n",
            "Epoch 166/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.5182 - accuracy: 0.2778\n",
            "Epoch 167/250\n",
            "11/11 [==============================] - 2s 228ms/step - loss: 2.5223 - accuracy: 0.2654\n",
            "Epoch 168/250\n",
            "11/11 [==============================] - 3s 230ms/step - loss: 2.4952 - accuracy: 0.2747\n",
            "Epoch 169/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.5112 - accuracy: 0.2593\n",
            "Epoch 170/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 2.5146 - accuracy: 0.2593\n",
            "Epoch 171/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.5056 - accuracy: 0.2623\n",
            "Epoch 172/250\n",
            "11/11 [==============================] - 2s 217ms/step - loss: 2.4608 - accuracy: 0.2840\n",
            "Epoch 173/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.4953 - accuracy: 0.2593\n",
            "Epoch 174/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.4936 - accuracy: 0.2809\n",
            "Epoch 175/250\n",
            "11/11 [==============================] - 3s 228ms/step - loss: 2.5070 - accuracy: 0.2809\n",
            "Epoch 176/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 2.4842 - accuracy: 0.2778\n",
            "Epoch 177/250\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 2.4970 - accuracy: 0.2654\n",
            "Epoch 178/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.4599 - accuracy: 0.2716\n",
            "Epoch 179/250\n",
            "11/11 [==============================] - 3s 230ms/step - loss: 2.4549 - accuracy: 0.2840\n",
            "Epoch 180/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.4555 - accuracy: 0.2623\n",
            "Epoch 181/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.4787 - accuracy: 0.2778\n",
            "Epoch 182/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.4584 - accuracy: 0.2778\n",
            "Epoch 183/250\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 2.4690 - accuracy: 0.2531\n",
            "Epoch 184/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.4726 - accuracy: 0.2809\n",
            "Epoch 185/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.4834 - accuracy: 0.2932\n",
            "Epoch 186/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.4608 - accuracy: 0.2840\n",
            "Epoch 187/250\n",
            "11/11 [==============================] - 2s 229ms/step - loss: 2.5251 - accuracy: 0.2654\n",
            "Epoch 188/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.4316 - accuracy: 0.2932\n",
            "Epoch 189/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.4269 - accuracy: 0.2685\n",
            "Epoch 190/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.4700 - accuracy: 0.2654\n",
            "Epoch 191/250\n",
            "11/11 [==============================] - 3s 226ms/step - loss: 2.4427 - accuracy: 0.2963\n",
            "Epoch 192/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 2.4033 - accuracy: 0.2870\n",
            "Epoch 193/250\n",
            "11/11 [==============================] - 2s 227ms/step - loss: 2.4209 - accuracy: 0.2716\n",
            "Epoch 194/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.4411 - accuracy: 0.2932\n",
            "Epoch 195/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 2.3815 - accuracy: 0.2870\n",
            "Epoch 196/250\n",
            "11/11 [==============================] - 3s 232ms/step - loss: 2.4183 - accuracy: 0.2840\n",
            "Epoch 197/250\n",
            "11/11 [==============================] - 2s 229ms/step - loss: 2.4310 - accuracy: 0.2685\n",
            "Epoch 198/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.4156 - accuracy: 0.3056\n",
            "Epoch 199/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.3967 - accuracy: 0.3025\n",
            "Epoch 200/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.3606 - accuracy: 0.3025\n",
            "Epoch 201/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.4567 - accuracy: 0.2716\n",
            "Epoch 202/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.4978 - accuracy: 0.2623\n",
            "Epoch 203/250\n",
            "11/11 [==============================] - 2s 228ms/step - loss: 2.4340 - accuracy: 0.2901\n",
            "Epoch 204/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.4251 - accuracy: 0.2747\n",
            "Epoch 205/250\n",
            "11/11 [==============================] - 3s 231ms/step - loss: 2.4182 - accuracy: 0.2901\n",
            "Epoch 206/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.4240 - accuracy: 0.2716\n",
            "Epoch 207/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.3803 - accuracy: 0.3025\n",
            "Epoch 208/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.3955 - accuracy: 0.2932\n",
            "Epoch 209/250\n",
            "11/11 [==============================] - 3s 225ms/step - loss: 2.4040 - accuracy: 0.2870\n",
            "Epoch 210/250\n",
            "11/11 [==============================] - 2s 227ms/step - loss: 2.3739 - accuracy: 0.3056\n",
            "Epoch 211/250\n",
            "11/11 [==============================] - 3s 229ms/step - loss: 2.3834 - accuracy: 0.2809\n",
            "Epoch 212/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.3267 - accuracy: 0.3210\n",
            "Epoch 213/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 2.3869 - accuracy: 0.2685\n",
            "Epoch 214/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.4015 - accuracy: 0.2870\n",
            "Epoch 215/250\n",
            "11/11 [==============================] - 2s 217ms/step - loss: 2.3563 - accuracy: 0.2809\n",
            "Epoch 216/250\n",
            "11/11 [==============================] - 2s 227ms/step - loss: 2.3644 - accuracy: 0.3086\n",
            "Epoch 217/250\n",
            "11/11 [==============================] - 2s 221ms/step - loss: 2.3617 - accuracy: 0.3056\n",
            "Epoch 218/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.3217 - accuracy: 0.2809\n",
            "Epoch 219/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.3592 - accuracy: 0.2901\n",
            "Epoch 220/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.3464 - accuracy: 0.3179\n",
            "Epoch 221/250\n",
            "11/11 [==============================] - 2s 216ms/step - loss: 2.3004 - accuracy: 0.3364\n",
            "Epoch 222/250\n",
            "11/11 [==============================] - 3s 234ms/step - loss: 2.3220 - accuracy: 0.2932\n",
            "Epoch 223/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.3229 - accuracy: 0.3302\n",
            "Epoch 224/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.3046 - accuracy: 0.3241\n",
            "Epoch 225/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.3725 - accuracy: 0.2870\n",
            "Epoch 226/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.3221 - accuracy: 0.3241\n",
            "Epoch 227/250\n",
            "11/11 [==============================] - 3s 232ms/step - loss: 2.2703 - accuracy: 0.3148\n",
            "Epoch 228/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.3023 - accuracy: 0.2932\n",
            "Epoch 229/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.3546 - accuracy: 0.3086\n",
            "Epoch 230/250\n",
            "11/11 [==============================] - 3s 225ms/step - loss: 2.3045 - accuracy: 0.3086\n",
            "Epoch 231/250\n",
            "11/11 [==============================] - 3s 233ms/step - loss: 2.3152 - accuracy: 0.3117\n",
            "Epoch 232/250\n",
            "11/11 [==============================] - 2s 228ms/step - loss: 2.3821 - accuracy: 0.2901\n",
            "Epoch 233/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.2824 - accuracy: 0.2963\n",
            "Epoch 234/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.2281 - accuracy: 0.3364\n",
            "Epoch 235/250\n",
            "11/11 [==============================] - 2s 225ms/step - loss: 2.3154 - accuracy: 0.3056\n",
            "Epoch 236/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.2621 - accuracy: 0.3210\n",
            "Epoch 237/250\n",
            "11/11 [==============================] - 3s 229ms/step - loss: 2.2598 - accuracy: 0.3025\n",
            "Epoch 238/250\n",
            "11/11 [==============================] - 2s 227ms/step - loss: 2.3248 - accuracy: 0.2932\n",
            "Epoch 239/250\n",
            "11/11 [==============================] - 2s 229ms/step - loss: 2.2896 - accuracy: 0.2870\n",
            "Epoch 240/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 2.2855 - accuracy: 0.3148\n",
            "Epoch 241/250\n",
            "11/11 [==============================] - 2s 222ms/step - loss: 2.2952 - accuracy: 0.3272\n",
            "Epoch 242/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 2.2391 - accuracy: 0.3333\n",
            "Epoch 243/250\n",
            "11/11 [==============================] - 2s 226ms/step - loss: 2.3063 - accuracy: 0.2870\n",
            "Epoch 244/250\n",
            "11/11 [==============================] - 2s 224ms/step - loss: 2.2377 - accuracy: 0.2932\n",
            "Epoch 245/250\n",
            "11/11 [==============================] - 2s 228ms/step - loss: 2.3327 - accuracy: 0.2994\n",
            "Epoch 246/250\n",
            "11/11 [==============================] - 3s 231ms/step - loss: 2.2900 - accuracy: 0.2901\n",
            "Epoch 247/250\n",
            "11/11 [==============================] - 2s 218ms/step - loss: 2.2550 - accuracy: 0.3086\n",
            "Epoch 248/250\n",
            "11/11 [==============================] - 2s 219ms/step - loss: 2.3068 - accuracy: 0.3210\n",
            "Epoch 249/250\n",
            "11/11 [==============================] - 2s 220ms/step - loss: 2.3127 - accuracy: 0.3333\n",
            "Epoch 250/250\n",
            "11/11 [==============================] - 2s 223ms/step - loss: 2.3353 - accuracy: 0.3117\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2458cbec3a0>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "m.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "m.fit(X_train, Y_train, epochs=250, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6f0Jxq_t9dk",
        "outputId": "3d0badae-f546-4c04-d1f0-fa5c78f683d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 1s 38ms/step - loss: 3.5903 - accuracy: 0.2222\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[3.5903286933898926, 0.2222222238779068]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m.evaluate(X_test,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qt7q6_FJSim"
      },
      "source": [
        "## Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDfXPj2DJV30"
      },
      "outputs": [],
      "source": [
        "#accuracy 95.5\n",
        "\"\"\" model is a simple feedforward neural network with three dense layers\"\"\"\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(128, input_shape=(len(X_train[0]),), activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(len(Y_train[0]), activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGYB1cRIJmFz",
        "outputId": "a3667001-5c1a-41b9-8e09-0729770b1097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "65/65 [==============================] - 1s 3ms/step - loss: 3.6093 - accuracy: 0.0556\n",
            "Epoch 2/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 3.4388 - accuracy: 0.1173\n",
            "Epoch 3/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 3.2788 - accuracy: 0.1605\n",
            "Epoch 4/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 3.0471 - accuracy: 0.2099\n",
            "Epoch 5/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 2.8275 - accuracy: 0.2593\n",
            "Epoch 6/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 2.5772 - accuracy: 0.3179\n",
            "Epoch 7/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 2.3261 - accuracy: 0.4012\n",
            "Epoch 8/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 2.1513 - accuracy: 0.4722\n",
            "Epoch 9/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 1.9809 - accuracy: 0.4475\n",
            "Epoch 10/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 1.8511 - accuracy: 0.4846\n",
            "Epoch 11/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 1.7138 - accuracy: 0.5154\n",
            "Epoch 12/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 1.5599 - accuracy: 0.5802\n",
            "Epoch 13/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 1.4690 - accuracy: 0.5926\n",
            "Epoch 14/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 1.3500 - accuracy: 0.6111\n",
            "Epoch 15/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 1.2439 - accuracy: 0.6327\n",
            "Epoch 16/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 1.1924 - accuracy: 0.6728\n",
            "Epoch 17/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 1.1933 - accuracy: 0.6914\n",
            "Epoch 18/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 1.0828 - accuracy: 0.7037\n",
            "Epoch 19/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.9709 - accuracy: 0.7438\n",
            "Epoch 20/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.8867 - accuracy: 0.7160\n",
            "Epoch 21/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.9118 - accuracy: 0.7623\n",
            "Epoch 22/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.8082 - accuracy: 0.7593\n",
            "Epoch 23/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.7329 - accuracy: 0.7963\n",
            "Epoch 24/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.7426 - accuracy: 0.7901\n",
            "Epoch 25/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.7424 - accuracy: 0.7901\n",
            "Epoch 26/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.6974 - accuracy: 0.7901\n",
            "Epoch 27/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.6378 - accuracy: 0.8272\n",
            "Epoch 28/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.5797 - accuracy: 0.8426\n",
            "Epoch 29/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.5726 - accuracy: 0.8364\n",
            "Epoch 30/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.5848 - accuracy: 0.8426\n",
            "Epoch 31/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.5768 - accuracy: 0.8272\n",
            "Epoch 32/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.6146 - accuracy: 0.8241\n",
            "Epoch 33/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3805 - accuracy: 0.8951\n",
            "Epoch 34/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.8488\n",
            "Epoch 35/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4785 - accuracy: 0.8457\n",
            "Epoch 36/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.8673\n",
            "Epoch 37/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3476 - accuracy: 0.8981\n",
            "Epoch 38/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4787 - accuracy: 0.8580\n",
            "Epoch 39/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4042 - accuracy: 0.8673\n",
            "Epoch 40/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8673\n",
            "Epoch 41/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3443 - accuracy: 0.9012\n",
            "Epoch 42/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4210 - accuracy: 0.8549\n",
            "Epoch 43/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.4578 - accuracy: 0.8302\n",
            "Epoch 44/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3925 - accuracy: 0.8642\n",
            "Epoch 45/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3370 - accuracy: 0.8951\n",
            "Epoch 46/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2959 - accuracy: 0.9167\n",
            "Epoch 47/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3605 - accuracy: 0.8889\n",
            "Epoch 48/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2953 - accuracy: 0.9012\n",
            "Epoch 49/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.9105\n",
            "Epoch 50/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2957 - accuracy: 0.9074\n",
            "Epoch 51/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3343 - accuracy: 0.8889\n",
            "Epoch 52/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.4085 - accuracy: 0.8765\n",
            "Epoch 53/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3112 - accuracy: 0.9136\n",
            "Epoch 54/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3157 - accuracy: 0.8951\n",
            "Epoch 55/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2681 - accuracy: 0.9167\n",
            "Epoch 56/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3595 - accuracy: 0.8951\n",
            "Epoch 57/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2967 - accuracy: 0.8889\n",
            "Epoch 58/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2048 - accuracy: 0.9444\n",
            "Epoch 59/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.8796\n",
            "Epoch 60/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2695 - accuracy: 0.9074\n",
            "Epoch 61/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2414 - accuracy: 0.9383\n",
            "Epoch 62/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.3117 - accuracy: 0.9228\n",
            "Epoch 63/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2632 - accuracy: 0.9136\n",
            "Epoch 64/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2739 - accuracy: 0.9105\n",
            "Epoch 65/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2393 - accuracy: 0.9228\n",
            "Epoch 66/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.9198\n",
            "Epoch 67/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1758 - accuracy: 0.9568\n",
            "Epoch 68/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2306 - accuracy: 0.9321\n",
            "Epoch 69/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2218 - accuracy: 0.9352\n",
            "Epoch 70/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1631 - accuracy: 0.9444\n",
            "Epoch 71/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9599\n",
            "Epoch 72/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.9568\n",
            "Epoch 73/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2938 - accuracy: 0.8920\n",
            "Epoch 74/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1774 - accuracy: 0.9537\n",
            "Epoch 75/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2564 - accuracy: 0.9228\n",
            "Epoch 76/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2001 - accuracy: 0.9506\n",
            "Epoch 77/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2379 - accuracy: 0.9352\n",
            "Epoch 78/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.9259\n",
            "Epoch 79/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2372 - accuracy: 0.9259\n",
            "Epoch 80/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.9228\n",
            "Epoch 81/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2145 - accuracy: 0.9259\n",
            "Epoch 82/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1547 - accuracy: 0.9475\n",
            "Epoch 83/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.9352\n",
            "Epoch 84/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 0.9414\n",
            "Epoch 85/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9414\n",
            "Epoch 86/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1337 - accuracy: 0.9599\n",
            "Epoch 87/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1579 - accuracy: 0.9537\n",
            "Epoch 88/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1916 - accuracy: 0.9414\n",
            "Epoch 89/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.9475\n",
            "Epoch 90/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1091 - accuracy: 0.9660\n",
            "Epoch 91/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1742 - accuracy: 0.9444\n",
            "Epoch 92/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.1425 - accuracy: 0.9537\n",
            "Epoch 93/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1485 - accuracy: 0.9475\n",
            "Epoch 94/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9537\n",
            "Epoch 95/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9321\n",
            "Epoch 96/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1834 - accuracy: 0.9414\n",
            "Epoch 97/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.9630\n",
            "Epoch 98/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.9475\n",
            "Epoch 99/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.9537\n",
            "Epoch 100/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.2015 - accuracy: 0.9383\n",
            "Epoch 101/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1384 - accuracy: 0.9660\n",
            "Epoch 102/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1750 - accuracy: 0.9444\n",
            "Epoch 103/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.9506\n",
            "Epoch 104/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1179 - accuracy: 0.9691\n",
            "Epoch 105/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1754 - accuracy: 0.9414\n",
            "Epoch 106/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.9506\n",
            "Epoch 107/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9383\n",
            "Epoch 108/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9568\n",
            "Epoch 109/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1772 - accuracy: 0.9352\n",
            "Epoch 110/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1720 - accuracy: 0.9444\n",
            "Epoch 111/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1263 - accuracy: 0.9599\n",
            "Epoch 112/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1631 - accuracy: 0.9444\n",
            "Epoch 113/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.9722\n",
            "Epoch 114/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9753\n",
            "Epoch 115/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1515 - accuracy: 0.9506\n",
            "Epoch 116/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1300 - accuracy: 0.9506\n",
            "Epoch 117/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0984 - accuracy: 0.9753\n",
            "Epoch 118/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1556 - accuracy: 0.9506\n",
            "Epoch 119/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9660\n",
            "Epoch 120/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.9753\n",
            "Epoch 121/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1629 - accuracy: 0.9352\n",
            "Epoch 122/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1284 - accuracy: 0.9568\n",
            "Epoch 123/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1140 - accuracy: 0.9599\n",
            "Epoch 124/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.9753\n",
            "Epoch 125/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1395 - accuracy: 0.9506\n",
            "Epoch 126/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.9722\n",
            "Epoch 127/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1085 - accuracy: 0.9660\n",
            "Epoch 128/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1686 - accuracy: 0.9568\n",
            "Epoch 129/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1254 - accuracy: 0.9753\n",
            "Epoch 130/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1413 - accuracy: 0.9444\n",
            "Epoch 131/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1967 - accuracy: 0.9414\n",
            "Epoch 132/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1447 - accuracy: 0.9568\n",
            "Epoch 133/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.9660\n",
            "Epoch 134/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1307 - accuracy: 0.9660\n",
            "Epoch 135/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1772 - accuracy: 0.9537\n",
            "Epoch 136/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1320 - accuracy: 0.9506\n",
            "Epoch 137/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1758 - accuracy: 0.9506\n",
            "Epoch 138/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1787 - accuracy: 0.9506\n",
            "Epoch 139/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9321\n",
            "Epoch 140/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.9475\n",
            "Epoch 141/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1403 - accuracy: 0.9630\n",
            "Epoch 142/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1486 - accuracy: 0.9660\n",
            "Epoch 143/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.9753\n",
            "Epoch 144/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0976 - accuracy: 0.9784\n",
            "Epoch 145/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.9537\n",
            "Epoch 146/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1409 - accuracy: 0.9444\n",
            "Epoch 147/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9722\n",
            "Epoch 148/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1503 - accuracy: 0.9475\n",
            "Epoch 149/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1260 - accuracy: 0.9599\n",
            "Epoch 150/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1334 - accuracy: 0.9599\n",
            "Epoch 151/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1668 - accuracy: 0.9475\n",
            "Epoch 152/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1372 - accuracy: 0.9568\n",
            "Epoch 153/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.9691\n",
            "Epoch 154/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1041 - accuracy: 0.9753\n",
            "Epoch 155/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9599\n",
            "Epoch 156/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1180 - accuracy: 0.9630\n",
            "Epoch 157/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1136 - accuracy: 0.9722\n",
            "Epoch 158/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1047 - accuracy: 0.9660\n",
            "Epoch 159/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1186 - accuracy: 0.9630\n",
            "Epoch 160/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1161 - accuracy: 0.9660\n",
            "Epoch 161/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1383 - accuracy: 0.9568\n",
            "Epoch 162/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.9722\n",
            "Epoch 163/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.9630\n",
            "Epoch 164/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.9599\n",
            "Epoch 165/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1602 - accuracy: 0.9383\n",
            "Epoch 166/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1269 - accuracy: 0.9537\n",
            "Epoch 167/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1031 - accuracy: 0.9753\n",
            "Epoch 168/200\n",
            "65/65 [==============================] - 0s 4ms/step - loss: 0.1061 - accuracy: 0.9691\n",
            "Epoch 169/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0920 - accuracy: 0.9784\n",
            "Epoch 170/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1175 - accuracy: 0.9660\n",
            "Epoch 171/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.9599\n",
            "Epoch 172/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1388 - accuracy: 0.9599\n",
            "Epoch 173/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1391 - accuracy: 0.9568\n",
            "Epoch 174/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1161 - accuracy: 0.9630\n",
            "Epoch 175/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1402 - accuracy: 0.9630\n",
            "Epoch 176/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9722\n",
            "Epoch 177/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1181 - accuracy: 0.9660\n",
            "Epoch 178/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1709 - accuracy: 0.9444\n",
            "Epoch 179/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9599\n",
            "Epoch 180/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1204 - accuracy: 0.9691\n",
            "Epoch 181/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0900 - accuracy: 0.9722\n",
            "Epoch 182/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1083 - accuracy: 0.9599\n",
            "Epoch 183/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9784\n",
            "Epoch 184/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1315 - accuracy: 0.9568\n",
            "Epoch 185/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1125 - accuracy: 0.9722\n",
            "Epoch 186/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0972 - accuracy: 0.9630\n",
            "Epoch 187/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1151 - accuracy: 0.9599\n",
            "Epoch 188/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1189 - accuracy: 0.9630\n",
            "Epoch 189/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.9722\n",
            "Epoch 190/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.9784\n",
            "Epoch 191/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0988 - accuracy: 0.9753\n",
            "Epoch 192/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9815\n",
            "Epoch 193/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1243 - accuracy: 0.9599\n",
            "Epoch 194/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1033 - accuracy: 0.9568\n",
            "Epoch 195/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9691\n",
            "Epoch 196/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.9784\n",
            "Epoch 197/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1111 - accuracy: 0.9630\n",
            "Epoch 198/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9784\n",
            "Epoch 199/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.1038 - accuracy: 0.9691\n",
            "Epoch 200/200\n",
            "65/65 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9784\n"
          ]
        }
      ],
      "source": [
        "sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMPjwvcAvoLl",
        "outputId": "3cb5fb21-3ad7-4ef3-bfe1-da0942377acf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2916 - accuracy: 0.8025\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.2915687561035156, 0.8024691343307495]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(X_test,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqPzPWyiRs37"
      },
      "source": [
        "### Sara"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6TQz5LW_r3g"
      },
      "outputs": [],
      "source": [
        "intents = json.loads(open('intents.json').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB7Mwd9T_rbZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE138g-GyIX8"
      },
      "outputs": [],
      "source": [
        "words = pickle.load(open('words.pkl','rb'))\n",
        "classes = pickle.load(open('classes.pkl','rb'))\n",
        "\n",
        "model = models.load_model('chatbot_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcDKbKw9yIu_"
      },
      "outputs": [],
      "source": [
        "def tokenize_lemmatize(sentence):\n",
        "    \"\"\"\n",
        "    Clean up the given sentence by tokenizing and lemmatizing the words.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence to clean up.\n",
        "\n",
        "    Returns:\n",
        "        list: The list of cleaned-up words from the sentence.\n",
        "    \"\"\"\n",
        "    sentence_words = nltk.word_tokenize(sentence)  # Tokenize the sentence into individual words\n",
        "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]  # Lemmatize the words\n",
        "    return sentence_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvYTdyHFyKgo"
      },
      "outputs": [],
      "source": [
        "def bag_of_words(sentence):\n",
        "    \"\"\"\n",
        "    Create a bag of words representation for the given sentence.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The bag of words representation as a numpy array.\n",
        "    \"\"\"\n",
        "    sentence_words = tokenize_lemmatize(sentence)  # Tokenize and lemmatize the words in the sentence\n",
        "    bag = [0] * len(vocab)  # Initialize the bag of words with zeros\n",
        "    for w in sentence_words:\n",
        "        for i, word in enumerate(vocab):\n",
        "            if word == w:\n",
        "                bag[i] = 1  # Set the corresponding index to 1 if the word is present in the vocab\n",
        "    return np.array(bag)  # Convert the bag of words to a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl3Qvzn0yMRN"
      },
      "outputs": [],
      "source": [
        "def predict_class(sentence):\n",
        "    # Convert the sentence into a bag of words representation\n",
        "    bow = bag_of_words(sentence)\n",
        "\n",
        "    # Use the model to predict the class probabilities for the input\n",
        "    res = model.predict(np.array([bow]))[0]\n",
        "\n",
        "    # Set the error threshold to determine which classes to consider\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "\n",
        "    # Filter out the classes with probabilities below the error threshold\n",
        "    result = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "\n",
        "    # Sort the filtered classes by probability in descending order\n",
        "    result.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Create a list of dictionaries with the intent and probability for each class\n",
        "    return_list = []\n",
        "    for r in result:\n",
        "        return_list.append({'intent': category[r[0]], 'probability': str(r[1])})\n",
        "\n",
        "    return return_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDCRPssUyOMY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "    # Get the tag (intent) from the first item in intents_list\n",
        "    tag = intents_list[0]['intent']\n",
        "\n",
        "    # Retrieve the list of intents from intents_json\n",
        "    list_of_intents = intents_json['intents']\n",
        "\n",
        "    # Iterate over each intent in the list_of_intents\n",
        "    for i in list_of_intents:\n",
        "        # Check if the tag matches the current intent's tag\n",
        "        if i['tag'] == tag:\n",
        "            # Select a random response from the current intent's responses\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "\n",
        "    # Return the selected response\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1rHvq-d4t0-"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.word_tokenize(\"Tokenize me\")\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1fJvpXayQ8W",
        "outputId": "cf372572-13d2-4193-b7bf-109571cef3b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT IN RUNNING\n",
            "hi\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "Good to see you again!\n"
          ]
        }
      ],
      "source": [
        "print(\"BOT IN RUNNING\")\n",
        "while True:\n",
        "    message = input(\"\")\n",
        "    ints = predict_class(message)\n",
        "    res = get_response(ints,intents)\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTeK1KpwoNgD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "esQGaIaqNr4r",
        "7T7fTj02TVvj"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}